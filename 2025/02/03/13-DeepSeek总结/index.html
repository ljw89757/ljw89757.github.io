<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="1. 参考 DeepSeek-V3技术报告：DeepSeek-V3&#x2F;DeepSeek_V3.pdf at main · deepseek-ai&#x2F;DeepSeek-V3 · GitHub  DeepSeek-R1技术报告：DeepSeek-R1&#x2F;DeepSeek_R1.pdf at main · deepseek-ai&#x2F;DeepSeek-R1 · GitHub">
<meta property="og:type" content="article">
<meta property="og:title" content="13-DeepSeek总结">
<meta property="og:url" content="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="睿智的ljw侠">
<meta property="og:description" content="1. 参考 DeepSeek-V3技术报告：DeepSeek-V3&#x2F;DeepSeek_V3.pdf at main · deepseek-ai&#x2F;DeepSeek-V3 · GitHub  DeepSeek-R1技术报告：DeepSeek-R1&#x2F;DeepSeek_R1.pdf at main · deepseek-ai&#x2F;DeepSeek-R1 · GitHub">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/641.webp">
<meta property="og:image" content="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/642.webp">
<meta property="og:image" content="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/643.webp">
<meta property="og:image" content="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/2025-02-04-00-49-49-image.png">
<meta property="og:image" content="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/2025-02-04-01-33-36-image.png">
<meta property="article:published_time" content="2025-02-03T03:49:05.000Z">
<meta property="article:modified_time" content="2025-02-14T02:37:03.705Z">
<meta property="article:author" content="Jiawei Li">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/641.webp">

<link rel="canonical" href="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>13-DeepSeek总结 | 睿智的ljw侠</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">睿智的ljw侠</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          13-DeepSeek总结
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-03 11:49:05" itemprop="dateCreated datePublished" datetime="2025-02-03T11:49:05+08:00">2025-02-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-14 10:37:03" itemprop="dateModified" datetime="2025-02-14T10:37:03+08:00">2025-02-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h5 id="1-参考"><a href="#1-参考" class="headerlink" title="1. 参考"></a>1. 参考</h5><ul>
<li><p>DeepSeek-V3技术报告：<a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">DeepSeek-V3&#x2F;DeepSeek_V3.pdf at main · deepseek-ai&#x2F;DeepSeek-V3 · GitHub</a></p>
</li>
<li><p>DeepSeek-R1技术报告：<a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">DeepSeek-R1&#x2F;DeepSeek_R1.pdf at main · deepseek-ai&#x2F;DeepSeek-R1 · GitHub</a></p>
</li>
<li><p>DeepSeekMath：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.03300">[2402.03300] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a></p>
</li>
<li><p>权重（大小足有671B，FP8精度）：<a target="_blank" rel="noopener" href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base">deepseek-ai&#x2F;DeepSeek-V3-Base · Hugging Face</a></p>
</li>
<li><p>【论文解读】DeepSeek-V3技术报告 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/14988009150">https://zhuanlan.zhihu.com/p/14988009150</a></p>
</li>
<li><p>【论文解读】DeepSeek-R1：通过强化学习提升LLM推理能力 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/19551355661">https://zhuanlan.zhihu.com/p/19551355661</a></p>
</li>
<li><p>【论文解读】DeepSeekMath：用GRPO改进PPO <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/14574329458">https://zhuanlan.zhihu.com/p/14574329458</a></p>
</li>
<li><p>八个问题，带你零基础了解DeepSeek <a target="_blank" rel="noopener" href="https://news.qq.com/rain/a/20250202A05WKP00">https://news.qq.com/rain/a/20250202A05WKP00</a></p>
</li>
</ul>
<hr>
<h5 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a>2. 背景</h5><p><img src="/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/641.webp"></p>
<p>DeepSeek已经发布13个大模型，并且都已开源。</p>
<p><img src="/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/642.webp"></p>
<p>最近引起全世界广泛关注的模型，主要是自研通用大模型 DeepSeek-V3、推理模型 DeepSeek-R1 。（DeepSeek-V3 对标GPT-4o，DeepSeek-R1对标o1）。</p>
<ul>
<li><p>DeepSeek-V3 是一个通用模型，日常常见的问题，都可以尝试使用 V3。</p>
</li>
<li><p>DeepSeek-R1 是一个推理模型，擅长处理复杂、需要多步思考的问题，适合做深度研究、解决代码问题、数学问题。</p>
</li>
</ul>
<p><img src="/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/643.webp"></p>
<p>目前Web和APP均免费。</p>
<ul>
<li><p>Web 端直接通过访问<a target="_blank" rel="noopener" href="https://chat.deepseek.com/">网址</a>对话。在对话框的左下角位置，可以选择是否开启“深度思考”模式。如果勾选，会使用 DeepSeek-R1 模型；如不勾选，则默认使用 DeepSeek-V3 。</p>
</li>
<li><p>App 直接在应用商店中搜索“DeepSeek”即可。在APP端，可以选择同时使用联网和推理功能。</p>
</li>
</ul>
<p>也可以通过多种渠道调用 DeepSeek 的API:</p>
<ol>
<li><p>DeepSeek开发者平台：访问 DeepSeek 控制台</p>
<p><a target="_blank" rel="noopener" href="https://platform.deepseek.com/%EF%BC%8C%E6%B3%A8%E5%86%8C%E7%99%BB%E5%BD%95%E5%B9%B6%E8%B4%AD%E4%B9%B0%E8%8E%B7%E5%8F%96%E7%9B%B8%E5%BA%94%E7%9A%84%E5%AF%86%E9%92%A5%E3%80%82%EF%BC%88%E4%B8%8D%E8%BF%87%EF%BC%8C%E8%BF%91%E6%9C%9F%E8%AF%A5%E5%B9%B3%E5%8F%B0%E6%AD%A3%E5%9C%A8%E7%BB%B4%E6%8A%A4%E5%BD%93%E4%B8%AD%EF%BC%89">https://platform.deepseek.com/，注册登录并购买获取相应的密钥。（不过，近期该平台正在维护当中）</a></p>
</li>
<li><p>英伟达 NIM 微服务：</p>
<p><a target="_blank" rel="noopener" href="https://build.nvidia.com/deepseek-ai/deepseek-r1%EF%BC%8C%E6%94%AF%E6%8C%81API%E8%B0%83%E7%94%A8">https://build.nvidia.com/deepseek-ai/deepseek-r1，支持API调用</a> DeepSeek-R1，需要使用邮箱注册账号。</p>
</li>
<li><p>微软 Azure：</p>
<p><a target="_blank" rel="noopener" href="https://ai.azure.com,微软/">https://ai.azure.com，微软</a> Azure 可以通过聊天操场，部署DeepSeek-R1，创建一个聊天机器人。</p>
</li>
<li><p>亚马逊 AWS：</p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/blogs/aws/deepseek-r1-models-now-available-on-aws%EF%BC%8CDeepSeek-R1">https://aws.amazon.com/cn/blogs/aws/deepseek-r1-models-now-available-on-aws，DeepSeek-R1</a> 现已在 Amazon Bedrock Marketplace 和 Amazon SageMaker JumpStart 中推出，还可以在 Amazon Bedrock Custom Model Import 和 Amazon EC2 实例来使用 DeepSeek-R1-Distill 模型。</p>
</li>
<li><p>硅基流动 SiliconCloud ：</p>
<p><a target="_blank" rel="noopener" href="https://siliconflow.cn/zh-cn/models">https://siliconflow.cn/zh-cn/models</a> ，上线了基于华为云昇腾云服务的 DeepSeek-V3、DeepSeek-R1，开发者可以直接调用 SiliconCloud API，价格与 DeepSeek 官方优惠期价格保持一致。</p>
</li>
<li><p>此外，Cerebras、Groq 也可以调用 DeepSeek-R1 的 API。</p>
</li>
</ol>
<hr>
<h5 id="3-DeepSeek的优势"><a href="#3-DeepSeek的优势" class="headerlink" title="3. DeepSeek的优势"></a>3. DeepSeek的优势</h5><p>特点：</p>
<ul>
<li><p>性能优秀：这两款模型的性能接近甚至在某些场景超越了“公认”的全球标杆公司OpenAI的最好产品</p>
</li>
<li><p>结合应用：模型发布后，上线DeepSeek的Web&#x2F;APP，可切身体验模型效果。</p>
</li>
<li><p>训练成本低，产品性价比高</p>
<ul>
<li><p>根据 DeepSeek 的官方技术报告，V3 的训练成本仅 557.6 万美元。OpenAI 虽然没有官方公布过 4o 的训练成本，但据OpenAI CEO Sam Altman 透露，GPT-4 的训练总计花费了约1亿美元。</p>
</li>
<li><p>V3 仅使用了 2048 个 H800 GPU、花费2个月训练完成，使用GPU的数量和训练时长颠覆传统认知。</p>
</li>
<li><p>R1 和 V3 都可以在 DeepSeek 官网上免费使用；API 的定价中，R1 输入部分的价格是 o1 的 1.82%，输出部分是 o1 的 3.65%；V3 输入部分的价格是 GPT-4o 的 1.12%，输出部分是 GPT-4o 的 2.8%。</p>
</li>
</ul>
</li>
<li><p>技术创新：DeepSeek-R1 的训练模式颠覆了常规认知。DeepSeek-R1 是首个验证了仅通过 RL（强化学习）无需 SFT （监督微调） 就能得到大幅推理能力增强和涌现的模型。这种训练方式大幅降低了数据标注成本，简化了训练流程，整体训练成本也得到了降低。</p>
</li>
<li><p>开源：目前没有其他在性能上对标 GPT-4o 和 o1 的开源模型。OpenAI 旗下主打的核心模型都没有开源，用户要使用必须通过APP或 API 调用。</p>
</li>
</ul>
<hr>
<h5 id="4-目前形势"><a href="#4-目前形势" class="headerlink" title="4. 目前形势"></a>4. 目前形势</h5><ul>
<li><p>中国AI公司做出真正的创新，美国科技大厂担心失去领先地位。</p>
<ul>
<li><p>在此之前，模型层面的技术革新虽然也并非罕见，但都是美国模型厂商率先推出、其他厂商跟进验证的节奏。这一次 DeepSeek 走到了前面。</p>
</li>
<li><p>DeepSeek 在模型训练和架构上都有创新，能够显著降低推理阶段的成本、提高效率。长期以来，AI发展依赖于计算能力的积累，可以说是超大规模者之间的竞赛。对比美国的竞争者，DeepSeek的创新实现了训练成本和使用价格上数量级的减少，美国公司领先市场的重要优势被削弱了。</p>
</li>
</ul>
</li>
<li><p>开源：生态若能星火燎原，将抢占美国公司市场。DeepSeek 的 R1 不仅通过技术报告公开了训练过程，还开源了模型的权重。DeepSeek的推理模型拥有高性能和低价格，使得开发者能将其用于越来越多的场景。最近，微软、英伟达、AWS都纷纷接入DeepSeek-R1。</p>
</li>
<li><p>大模型相关的美国科技股受到巨大冲击。英伟达股价大跌，似乎暗示了 DeepSeek 的真实威胁。因为DeepSeek 的路线一定程度上说明，无需最强算力也能训练出高性能大模型，而且 DeepSeek 把高性能模型开源的路线可能让更多公司放弃训练模型，冲击了英伟达核心算力产品（GPU）的需求，影响股价。并且，市场担忧 DeepSeek 的成功冲击 OpenAI 等美国重点科技公司的市场前景，尤其是闭源模型方向。</p>
</li>
</ul>
<hr>
<h5 id="5-DeepSeek未来可能迭代方向"><a href="#5-DeepSeek未来可能迭代方向" class="headerlink" title="5. DeepSeek未来可能迭代方向"></a>5. DeepSeek未来可能迭代方向</h5><ul>
<li><p>未来的创新点可能还是会围绕着<strong>成本、性能</strong>这两大核心要素。</p>
</li>
<li><p>多模态能力补齐。</p>
<ul>
<li>除夕凌晨，DeepSeek新发布的 DeepSeek-Janus-Pro 模型是一个多模态模型，同时拥有视觉理解和视觉生成的能力。但 Janus 系列模型都是小参数量模型，如何通过 Janus 创新的模型框架训练出一个大参数量的多模态模型，可能是未来的重点之一。</li>
</ul>
</li>
<li><p>DeepSeek 在2025年1月终于推出面向 C 端用户的 APP 产品，可能未来会探索&#x2F;合作更多应用。</p>
</li>
</ul>
<hr>
<h5 id="6-可能会带来的影响"><a href="#6-可能会带来的影响" class="headerlink" title="6. 可能会带来的影响"></a>6. 可能会带来的影响</h5><ul>
<li><p>国内AI公司面临进一步限制</p>
<ul>
<li><p>芯片制裁可能更严重：DeepSeek 的低成本训练成果，可能会让美国进一步收缩可供出口的芯片型号。未来，国内模型厂商可用的 GPU 型号越来越少，代际越来越旧。</p>
</li>
<li><p>模型和应用层面的封锁也会随之而来：由于隐私、数据合规等质疑，一些国家和地区已经要求 DeepSeek 停止服务。Twitter上，一些 AI 科普类博主从之前的无脑捧吹 DeepSeek 的帖子，已经转变为教用户 “如何本地化部署一个 DeepSeek R1 来保护自己的数据” 这样的帖子。</p>
</li>
</ul>
</li>
<li><p>全球AI生态的竞争可能会被重塑</p>
<ul>
<li>DeepSeek 得到市场认可，一定程度上说明，算法效率、经济高效将成为未来竞争中的核心要素。DeepSeek 正推动 AI 行业从“算力军备竞赛”转向“算法效率战争”，AI技术进一步普惠化。那些以往以“算力为重”的公司将要重新审视自己的战略。</li>
</ul>
</li>
<li><p>硅谷巨头们急迫重新领先</p>
<ul>
<li>技术上进行革新、重新夺取领先地位的紧迫感，会笼罩着美国的科技巨头们。据称，目前 Google、Apple、Meta 等公司，已经纷纷开始深度研究 DeepSeek。尽快推出下一个代际的领先模型，是硅谷各家的当务之急。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="7-可能面临的有利影响和发展方向"><a href="#7-可能面临的有利影响和发展方向" class="headerlink" title="7. 可能面临的有利影响和发展方向"></a>7. 可能面临的有利影响和发展方向</h5><p>有利影响：</p>
<ul>
<li><p>随着国际芯片制裁加剧，国内市场对国产GPU和AI芯片的需求将上升，有机会填补这一空白。</p>
</li>
<li><p>数据隐私和合规要求推动本地化部署，可提供定制化解决方案。</p>
</li>
<li><p>可优化算法与硬件的协同，提升整体性能。参与国产AI生态建设，推动从硬件到软件的自主可控。</p>
</li>
</ul>
<p>发展方向：提供定制化解决方案，针对数据隐私和合规需求，提供本地化部署支持。</p>
<hr>
<h5 id="8-技术细节"><a href="#8-技术细节" class="headerlink" title="8. 技术细节"></a>8. 技术细节</h5><h6 id="8-1-DeepSeek-V3-使用-MoE-架构："><a href="#8-1-DeepSeek-V3-使用-MoE-架构：" class="headerlink" title="8.1 DeepSeek-V3 使用 MoE 架构："></a>8.1 DeepSeek-V3 使用 MoE 架构：</h6><p>MoE (Mixture-of-Experts) 模型，即混合专家模型，是一种将多个小型专家模型组合起来，共同完成任务的架构。每个专家模型只处理部分输入，从而提高模型的效率和扩展性。</p>
<p>DeepSeek-V3 使用 MoE 架构的原因主要有以下几点：</p>
<ul>
<li>提高模型容量：MoE 可以显著增加模型的参数数量，而无需像稠密模型那样增加计算量。这使得模型可以学习更复杂的模式和关系，提高模型的性能。</li>
<li>降低训练成本：虽然 MoE 模型的总参数量很大，但每次激活的参数量却很小，从而减少了训练所需的计算资源。</li>
<li>提高推理效率： 在推理时，只有部分专家被激活，从而降低了推理的计算成本。</li>
</ul>
<hr>
<h6 id="8-2-DeepSeek-V3-使用Multi-Head-Latent-Attention-MLA"><a href="#8-2-DeepSeek-V3-使用Multi-Head-Latent-Attention-MLA" class="headerlink" title="8.2 DeepSeek-V3 使用Multi-Head Latent Attention (MLA)"></a>8.2 DeepSeek-V3 使用Multi-Head Latent Attention (MLA)</h6><p>Multi-Head Latent Attention (MLA)，即多头潜在注意力机制，旨在减少推理阶段的内存占用。MLA 和传统的 MHA 的主要区别在于：</p>
<ul>
<li>KV 缓存：在传统的 MHA 中，每个 token 的 Key 和 Value 向量都需要缓存起来，这在长文本生成中会占用大量的显存。而 MLA 通过低秩压缩技术，将 Key 和 Value 向量压缩成低维度的潜在向量，只需缓存压缩后的表示，从而显著减少 KV 缓存的内存占用。</li>
<li>通俗类比：想象在听一场演讲，传统的注意力机制需要记住每个时间点的所有细节（就像记录每一秒的录音）。而 MLA 则只提取关键信息（就像只记录每个重点句子），这样需要记忆的信息就大大减少了。</li>
</ul>
<p><img src="/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/2025-02-04-00-49-49-image.png"></p>
<hr>
<h6 id="8-3-DeepSeek-V3-的无辅助损失负载均衡策略"><a href="#8-3-DeepSeek-V3-的无辅助损失负载均衡策略" class="headerlink" title="8.3 DeepSeek-V3 的无辅助损失负载均衡策略"></a>8.3 DeepSeek-V3 的无辅助损失负载均衡策略</h6><p>在 MoE 模型中，如果专家负载不均衡，会导致以下问题：</p>
<ul>
<li>路由崩溃：某些专家可能会被过度使用，而其他专家则被闲置。</li>
<li>计算效率降低：过度使用的专家会成为性能瓶颈，而闲置的专家则浪费了计算资源。</li>
</ul>
<p>传统 MoE 模型为了实现专家间的负载均衡，通常引入辅助损失。但辅助损失会对模型的主要优化目标产生干扰，可能导致性能下降。为了解决这个问题，DeepSeek-V3 提出了无辅助损失的负载均衡策略。该策略的核心思想是：</p>
<ol>
<li>引入偏置项：为每个专家引入一个偏置项，并将这个偏置项添加到亲和度得分中，从而影响路由决策。</li>
<li>动态调整偏置项：在训练过程中，如果一个专家被过度使用，则减小其偏置项；如果一个专家被闲置，则增大其偏置项。这样，可以动态调整专家负载，使其达到均衡。</li>
<li>不使用辅助损失：偏置项只用于路由，而不影响最终的门控值计算，从而避免了传统辅助损失对模型性能的损害。</li>
</ol>
<p>类比： 想象在一个餐厅里，有多个厨师。如果顾客都点同样的菜，那么只有一个厨师会很忙，而其他厨师则没事做。为了解决这个问题，你可以告诉顾客：「今天点这道菜可以打折」，或者「点其他菜可以更快上菜」。这样，就可以引导顾客点不同的菜，让所有厨师都忙起来。DeepSeek-V3 的无辅助损失负载均衡策略就类似于这个引导顾客点不同菜的策略。</p>
<hr>
<h6 id="8-4-DeepSeek-V3-使用-Multi-Token-Prediction-MTP-提高模型性能"><a href="#8-4-DeepSeek-V3-使用-Multi-Token-Prediction-MTP-提高模型性能" class="headerlink" title="8.4 DeepSeek-V3 使用 Multi-Token Prediction (MTP) 提高模型性能"></a>8.4 DeepSeek-V3 使用 Multi-Token Prediction (MTP) 提高模型性能</h6><p>在传统的语言模型训练中，模型通常只预测下一个标记。MTP 则是在每个位置上，让模型预测多个未来的标记。</p>
<ul>
<li><p>实现方法：引入了多个预测模块，每个模块负责预测不同深度（未来第几个）的标记。在训练过程中，对每个深度的预测计算损失，然后将这些<strong>损失平均</strong>，作为整体的 MTP 损失。</p>
</li>
<li><p>好处：增加训练信号：通过预测多个 token，模型可以获得更密集的训练信号，提高数据效率。增强预规划能力：MTP 可以帮助模型更好地预先规划未来 token 的表示，使其更好地捕捉长距离依赖关系，从而提高模型的性能。</p>
</li>
</ul>
<hr>
<h6 id="8-5-DeepSeek-V3-使用-FP8-训练混合精度训练"><a href="#8-5-DeepSeek-V3-使用-FP8-训练混合精度训练" class="headerlink" title="8.5 DeepSeek-V3 使用 FP8 训练混合精度训练"></a>8.5 DeepSeek-V3 使用 FP8 训练混合精度训练</h6><p>在深度学习训练中，使用低精度浮点数（如 FP16、FP8）可以降低计算和存储需求，提高训练速度。但低精度可能导致数值不稳定和模型性能下降。</p>
<ul>
<li><p>解决方案：</p>
<ul>
<li><p>精细量化策略。细粒度量化：对张量进行小块划分（如 1×128 或 128×128），对每个小块单独计算缩放因子，进行量化，减少了量化误差带来的影响。</p>
</li>
<li><p>提高积累精度。增加乘加运算的精度：在进行矩阵乘法累加时，将部分结果提升到更高精度（如 FP32）进行积累，减少了因低精度引入的误差。</p>
</li>
<li><p>混合精度训练框架。保留关键操作的高精度：对于敏感的操作，如嵌入层、归一化层等，仍然使用高精度计算，确保训练的稳定性。</p>
</li>
</ul>
</li>
<li><p>DeepSeek-V3 使用 FP8 训练的主要原因是：</p>
<ul>
<li><p>加速训练：FP8 格式可以加速 GEMM (General Matrix Multiplication) 等核心计算操作，从而提高训练速度。</p>
</li>
<li><p>减少内存使用：FP8 格式可以显著减少内存占用，使得在有限的显存中训练更大的模型成为可能。</p>
</li>
<li><p>突破硬件限制：目前 NVIDIA H100 及以上的 GPU 已经对 FP8 有了较好的支持，而 DeepSeek-V3 使用了 FP8 混合精度训练，使其可以更好地利用硬件性能。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h6 id="8-6-DeepSeek-V3-的-DualPipe-算法解决通信瓶颈"><a href="#8-6-DeepSeek-V3-的-DualPipe-算法解决通信瓶颈" class="headerlink" title="8.6 DeepSeek-V3 的 DualPipe 算法解决通信瓶颈"></a>8.6 DeepSeek-V3 的 DualPipe 算法解决通信瓶颈</h6><p>DualPipe 算法是 DeepSeek-V3 中用于解决通信瓶颈的一种高效流水线并行算法。它的核心思想是通过重叠前向和后向计算与通信，从而减少流水线气泡，提高训练效率。</p>
<p>具体来说，DualPipe 算法：</p>
<ol>
<li>双向流水线：同时从流水线的两端输入 micro-batch，实现双向并行计算。</li>
<li>计算-通信重叠：将每个 chunk 分成多个部分，并调整 GPU SM 用于通信和计算的比例，使得计算和通信可以相互重叠。</li>
<li>减少流水线气泡： 通过双向流水线和计算-通信重叠，减少了流水线中的等待时间，从而提高了训练效率。</li>
</ol>
<p>类比：可以将 DualPipe 算法想象成一个生产线。传统的流水线是按顺序一个一个地处理产品，而 DualPipe 算法是同时从两个方向处理产品，并让不同环节同时进行，从而提高了生产效率。</p>
<hr>
<h6 id="8-7-DeepSeek-V3-与传统的知识蒸馏的不同"><a href="#8-7-DeepSeek-V3-与传统的知识蒸馏的不同" class="headerlink" title="8.7 DeepSeek-V3 与传统的知识蒸馏的不同"></a>8.7 DeepSeek-V3 与传统的知识蒸馏的不同</h6><p>DeepSeek-V3 使用了一种特殊的知识蒸馏方法，从 DeepSeek-R1 模型中提炼推理能力。与传统的知识蒸馏不同，DeepSeek-V3 的知识蒸馏：</p>
<ul>
<li>重点在于推理模式：不是简单地模仿 R1 的输出，而是将 R1 的反思和验证模式融入到 DeepSeek-V3 中。</li>
<li>生成多样化的 SFT 数据：通过让 R1 生成带有反思和验证的响应，为 DeepSeek-V3 提供更高质量的 SFT 数据。</li>
<li>在强化学习中运用：在 RL 阶段，通过高温度采样生成融合了 R1 和原始数据的响应，使模型学习 R1 的推理模式。</li>
</ul>
<hr>
<h6 id="8-8-DeepSeek-V3-使用-tile-wise-和-block-wise-量化"><a href="#8-8-DeepSeek-V3-使用-tile-wise-和-block-wise-量化" class="headerlink" title="8.8 DeepSeek-V3 使用 tile-wise 和 block-wise 量化"></a>8.8 DeepSeek-V3 使用 <code>tile-wise</code> 和 <code>block-wise</code> 量化</h6><p><code>tile-wise</code> 和 <code>block-wise</code> 量化是用于提高 FP8 量化精度的两种细粒度量化方法：</p>
<ul>
<li>tile-wise 量化：对于激活值（activations），将 1xNe 的元素分组成一个 tile，然后在 tile 内进行缩放（scaling）。这里的 Ne 通常是 128，所以就是一个 1x128 的 tile。这意味着，对于每一个 token，每 128 个通道（channel）会被分成一组。</li>
<li>block-wise 量化：对于权重（weights），将 Ne x Ne 的元素分组成一个 block，然后在 block 内进行缩放。这里的 Ne 也通常是 128，所以就是一个 128x128 的 block。这意味着，每 128 个输入通道和 128 个输出通道会被分组成一个 block。</li>
</ul>
<p>这种细粒度的量化方法可以更好地适应激活值和权重中的异常值，提高量化的精度，从而使模型能够更有效地利用低精度的 FP8 格式进行训练。</p>
<p>类比：可以将 <code>tile-wise</code> 和 <code>block-wise</code> 量化想象成在打包货物时，不是把所有货物都放在一个大箱子里，而是将它们根据形状、大小等进行分类，然后分别打包。这样可以更好地利用空间，减少货物之间的挤压和碰撞。</p>
<hr>
<h6 id="8-9-DeepSeek-R1-Zero-研究纯强化学习（RL）在-LLM-推理能力上的应用"><a href="#8-9-DeepSeek-R1-Zero-研究纯强化学习（RL）在-LLM-推理能力上的应用" class="headerlink" title="8.9 DeepSeek-R1-Zero 研究纯强化学习（RL）在 LLM 推理能力上的应用"></a>8.9 DeepSeek-R1-Zero 研究纯强化学习（RL）在 LLM 推理能力上的应用</h6><p>直接在基础模型上应用强化学习，不使用任何 SFT 数据。探索 LLM 在纯 RL 环境下的自演化过程，使其自主发展推理能力。</p>
<ul>
<li><p>近年来，LLM 在各个领域都取得了显著进展，但推理能力仍有提升空间。</p>
</li>
<li><p>之前的研究大多依赖于大量的监督式微调（SFT）数据，但获取高质量的 SFT 数据成本高昂。</p>
</li>
<li><p>OpenAI 的 o1 系列模型通过增加思维链（Chain-of-Thought, CoT）推理过程的长度来提升推理能力，但如何有效进行测试时（test-time）扩展仍是开放问题。</p>
</li>
<li><p>一些研究尝试使用基于过程的奖励模型、强化学习和搜索算法来解决推理问题，但没有达到 OpenAI 的 o1 系列模型的通用推理性能水平。</p>
</li>
</ul>
<hr>
<h6 id="8-10-DeepSeek-R1-Zero-的「顿悟」时刻-aha-moment"><a href="#8-10-DeepSeek-R1-Zero-的「顿悟」时刻-aha-moment" class="headerlink" title="8.10 DeepSeek-R1-Zero 的「顿悟」时刻 aha moment"></a>8.10 DeepSeek-R1-Zero 的「顿悟」时刻 aha moment</h6><p>在大规模强化学习中，模型的「思考过程」会不断与最终的正确率奖励相互作用。当模型最初得出的答案并未得到较高奖励时，它会在后续的推理中「回头反省」，尝试补充或修正先前的思路，从而获得更高的奖励。随着强化学习的迭代，这种「主动回溯、推翻先前想法并重新推理」的行为逐渐巩固，便在输出中表现为所谓的「aha moment」。本质上，这是RL为模型「留出了」足够的思考和试错空间，当模型自行发现更优思路时，就会出现类似人类「恍然大悟」的瞬间。</p>
<p>这也展示了 RL 的强大潜力，它可以让模型在没有明确指导的情况下，自主学习并改进。</p>
<hr>
<h6 id="8-11-DeepSeek-R1相比DeepSeek-R1-Zero-做出的改进"><a href="#8-11-DeepSeek-R1相比DeepSeek-R1-Zero-做出的改进" class="headerlink" title="8.11 DeepSeek-R1相比DeepSeek-R1-Zero 做出的改进"></a>8.11 DeepSeek-R1相比DeepSeek-R1-Zero 做出的改进</h6><ul>
<li><p>引入冷启动数据：使用了数千条带详细推理过程（长CoT）的数据先做一次SFT，让模型初始时就具备一定可读性与写作风格。</p>
</li>
<li><p>分阶段RL：在推理收敛后，通过拒绝采样等手段获得更多优质监督样本，再进行SFT，再全场景RL，不断修正模型的正确性与通用能力。</p>
</li>
<li><p>语言一致性奖励：避免模型出现大量的拼写或中英文混杂，从而保证可读性。</p>
</li>
</ul>
<hr>
<h6 id="8-12-DeepSeek-R1-要使用冷启动数据"><a href="#8-12-DeepSeek-R1-要使用冷启动数据" class="headerlink" title="8.12 DeepSeek-R1 要使用冷启动数据"></a>8.12 DeepSeek-R1 要使用冷启动数据</h6><p>DeepSeek-R1 使用冷启动数据的主要目的是为了解决 DeepSeek-R1-Zero 在训练早期出现的训练不稳定问题。相比于直接在基础模型上进行 RL，使用少量的 SFT 数据进行冷启动，可以让模型更快地进入稳定训练阶段：</p>
<ul>
<li>可读性：冷启动数据使用更易于理解的格式，输出内容更适合人类阅读，避免了 DeepSeek-R1-Zero 输出的语言混合、格式混乱等问题。</li>
<li>潜在性能：通过精心设计冷启动数据的模式，可以引导模型产生更好的推理能力。</li>
<li>稳定训练：使用 SFT 数据作为起始点，可以避免RL 训练早期阶段的不稳定问题。</li>
</ul>
<hr>
<h6 id="8-13-DeepSeek-R1-的多阶段训练框架中每个阶段的侧重点"><a href="#8-13-DeepSeek-R1-的多阶段训练框架中每个阶段的侧重点" class="headerlink" title="8.13 DeepSeek-R1 的多阶段训练框架中每个阶段的侧重点"></a>8.13 DeepSeek-R1 的多阶段训练框架中每个阶段的侧重点</h6><ul>
<li><p>冷启动（Cold Start）：使用少量高质量的 CoT 数据对基础模型进行微调，作为 RL 训练的初始起点。侧重点是让模型掌握基本的 CoT 推理能力，并使模型的输出更具可读性。</p>
</li>
<li><p>推理导向的强化学习（Reasoning-oriented RL）：在冷启动模型的基础上进行 RL 训练，侧重点是提升模型在推理任务上的性能。在这个阶段，会引入语言一致性奖励，以减少推理过程中的语言混合问题。</p>
</li>
<li><p>拒绝采样和监督微调（Rejection Sampling and SFT）：使用上一阶段的 RL 模型进行拒绝采样，生成高质量的推理和非推理数据，并用这些数据对模型进行微调。侧重点是提升模型的综合能力，使其在写作、事实问答等多种任务上表现良好。</p>
</li>
<li><p>所有场景下的强化学习（RL for all scenarios）：在上一阶段 SFT 模型的基础上进行 RL 训练，侧重点是使模型在所有场景下都能表现良好，包括推理任务和非推理任务，并且保证模型的安全性和无害性。</p>
</li>
</ul>
<hr>
<h6 id="8-14-DeepSeekMath-Corpus-的构建过程，需要迭代式地收集数据"><a href="#8-14-DeepSeekMath-Corpus-的构建过程，需要迭代式地收集数据" class="headerlink" title="8.14 DeepSeekMath Corpus 的构建过程，需要迭代式地收集数据"></a>8.14 DeepSeekMath Corpus 的构建过程，需要迭代式地收集数据</h6><p>迭代式地收集数据可以帮助不断优化 FastText 模型，从而发现更多高质量的数学网页。</p>
<ul>
<li>第一轮，使用 OpenWebMath 作为种子数据训练了一个 FastText 模型。但是这个模型只学到了一部分数学网页的特征，还有很多数学网页没有被识别出来。</li>
<li>为了让 FastText 模型更强大，分析第一轮收集到的网页的来源，找到一些可能包含更多数学网页的网站，比如 <a href="https://link.zhihu.com/?target=http://mathoverflow.net">http://mathoverflow.net</a>。然后，人工标注这些网站中的数学网页，将他们加入到种子数据中，用新的种子数据训练新的 FastText 模型，这个新模型就可以识别更多的数学网页。</li>
<li>通过多轮迭代，FastText 模型越来越强大，可以找到更多高质量的数学网页。就像我们学习一个新知识一样，先从简单的概念入手，然后不断深入，才能全面掌握。</li>
</ul>
<hr>
<h6 id="8-15-DeepSeekMath-Base-从-DeepSeek-Coder-Base-模型开始进行数学预训练"><a href="#8-15-DeepSeekMath-Base-从-DeepSeek-Coder-Base-模型开始进行数学预训练" class="headerlink" title="8.15 DeepSeekMath-Base 从 DeepSeek-Coder-Base 模型开始进行数学预训练"></a>8.15 DeepSeekMath-Base 从 DeepSeek-Coder-Base 模型开始进行数学预训练</h6><p>使用 DeepSeek-Coder-Base-v1.5 7B 作为初始化模型，并在 DeepSeekMath Corpus 上进行预训练。因为发现代码训练有助于提升模型的推理能力，特别是数学推理能力。具体来说，代码训练可以让模型更好地理解逻辑和结构化的思维，这对于解决数学问题至关重要。先进行代码训练，再进行数学训练，可以获得更好的数学推理能力。</p>
<ul>
<li>逻辑思维：代码的执行需要精确的逻辑，这与数学推理非常相似。代码训练可以帮助模型学习和掌握逻辑思维的能力，从而更好地解决数学问题。</li>
<li>结构化思维：代码具有严谨的结构，这与数学证明的结构类似。代码训练可以让模型更好地理解和运用结构化思维，这有助于模型更好地分析和解决数学问题。</li>
<li>抽象能力：代码中的变量和函数是对现实世界事物的抽象，这与数学中的符号和公式类似。代码训练可以帮助模型提高抽象能力，从而更好地理解数学概念和公式。</li>
<li>解决问题的能力：代码的目的是解决问题，数学的目的是解决数学问题，这个过程都需要很强的解决问题的能力，代码训练可以帮助模型提高解决问题的能力，从而更好地解决数学问题。</li>
</ul>
<hr>
<h6 id="8-16-DeepSeekMath-RL-通过-GRPO-强化学习进一步优化的模型"><a href="#8-16-DeepSeekMath-RL-通过-GRPO-强化学习进一步优化的模型" class="headerlink" title="8.16 DeepSeekMath-RL 通过 GRPO 强化学习进一步优化的模型"></a>8.16 DeepSeekMath-RL 通过 GRPO 强化学习进一步优化的模型</h6><p>GRPO 算法减少计算资源消耗。GRPO 的核心在于它避免了像 PPO 那样训练一个额外的 value 模型，而是通过 group scores 来估计 baseline。</p>
<ul>
<li><p>PPO 需要训练一个额外的 value 模型来估计状态的价值，这个 value 模型通常和 policy 模型规模相当，所以训练过程很耗费计算资源。</p>
</li>
<li><p>GRPO 则不同，它不需要训练额外的 value 模型。对于每个问题，GRPO 会从 policy 模型中采样多个输出，然后将这些输出的奖励值进行归一化，并以此作为 baseline。这样，GRPO 可以省去训练 value 模型的开销，从而大大减少计算资源的消耗。</p>
</li>
<li><p>更形象地说，PPO 要给每个学生都找一个辅导老师（value model），而 GRPO 是让学生之间互相评价（group score），然后进行自我调整，显然后者更节省资源。</p>
</li>
</ul>
<p><img src="/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/2025-02-04-01-33-36-image.png"></p>
<hr>
<h6 id="8-17-RL-的未来方向"><a href="#8-17-RL-的未来方向" class="headerlink" title="8.17 RL 的未来方向"></a>8.17 RL 的未来方向</h6><p>总的来说，未来的研究方向将集中于如何让 RL 更有效地利用数据，更可靠地学习，以及更准确地评估。</p>
<ul>
<li><p>数据源：</p>
<ul>
<li><p>探索更多样的训练数据，包括分布外的问题和高级解码策略；</p>
</li>
<li><p>优化模型探索效率。</p>
</li>
</ul>
</li>
<li><p>算法：</p>
<ul>
<li>开发更鲁棒的 RL 算法，以应对训练信号中的噪声，比如使用弱监督方法。</li>
</ul>
</li>
<li><p>奖励函数：</p>
<ul>
<li><p>提高奖励函数的泛化能力，使其能够处理分布外问题和高级解码输出；</p>
</li>
<li><p>反映奖励函数的不确定性，将其与弱监督方法连接起来；</p>
</li>
<li><p>构建高质量的过程奖励模型，为推理过程提供细粒度的训练信号。</p>
</li>
</ul>
</li>
</ul>
<hr>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/01/27/12-Docker%E5%AE%B9%E5%99%A8%E4%B8%AD%E8%A7%A3%E5%86%B3SharedMemory%E4%B8%8D%E8%B6%B3%E9%97%AE%E9%A2%98/" rel="prev" title="12-Docker容器中解决Shared Memory不足问题">
      <i class="fa fa-chevron-left"></i> 12-Docker容器中解决Shared Memory不足问题
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/02/05/14-Git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/" rel="next" title="14-Git基本操作流程及注意事项">
      14-Git基本操作流程及注意事项 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-5"><a class="nav-link" href="#1-%E5%8F%82%E8%80%83"><span class="nav-number">1.</span> <span class="nav-text">1. 参考</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-%E8%83%8C%E6%99%AF"><span class="nav-number">2.</span> <span class="nav-text">2. 背景</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-DeepSeek%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="nav-number">3.</span> <span class="nav-text">3. DeepSeek的优势</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-%E7%9B%AE%E5%89%8D%E5%BD%A2%E5%8A%BF"><span class="nav-number">4.</span> <span class="nav-text">4. 目前形势</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#5-DeepSeek%E6%9C%AA%E6%9D%A5%E5%8F%AF%E8%83%BD%E8%BF%AD%E4%BB%A3%E6%96%B9%E5%90%91"><span class="nav-number">5.</span> <span class="nav-text">5. DeepSeek未来可能迭代方向</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#6-%E5%8F%AF%E8%83%BD%E4%BC%9A%E5%B8%A6%E6%9D%A5%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">6.</span> <span class="nav-text">6. 可能会带来的影响</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-%E5%8F%AF%E8%83%BD%E9%9D%A2%E4%B8%B4%E7%9A%84%E6%9C%89%E5%88%A9%E5%BD%B1%E5%93%8D%E5%92%8C%E5%8F%91%E5%B1%95%E6%96%B9%E5%90%91"><span class="nav-number">7.</span> <span class="nav-text">7. 可能面临的有利影响和发展方向</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#8-%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82"><span class="nav-number">8.</span> <span class="nav-text">8. 技术细节</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#8-1-DeepSeek-V3-%E4%BD%BF%E7%94%A8-MoE-%E6%9E%B6%E6%9E%84%EF%BC%9A"><span class="nav-number">8.1.</span> <span class="nav-text">8.1 DeepSeek-V3 使用 MoE 架构：</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-2-DeepSeek-V3-%E4%BD%BF%E7%94%A8Multi-Head-Latent-Attention-MLA"><span class="nav-number">8.2.</span> <span class="nav-text">8.2 DeepSeek-V3 使用Multi-Head Latent Attention (MLA)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-3-DeepSeek-V3-%E7%9A%84%E6%97%A0%E8%BE%85%E5%8A%A9%E6%8D%9F%E5%A4%B1%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E7%AD%96%E7%95%A5"><span class="nav-number">8.3.</span> <span class="nav-text">8.3 DeepSeek-V3 的无辅助损失负载均衡策略</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-4-DeepSeek-V3-%E4%BD%BF%E7%94%A8-Multi-Token-Prediction-MTP-%E6%8F%90%E9%AB%98%E6%A8%A1%E5%9E%8B%E6%80%A7%E8%83%BD"><span class="nav-number">8.4.</span> <span class="nav-text">8.4 DeepSeek-V3 使用 Multi-Token Prediction (MTP) 提高模型性能</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-5-DeepSeek-V3-%E4%BD%BF%E7%94%A8-FP8-%E8%AE%AD%E7%BB%83%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6%E8%AE%AD%E7%BB%83"><span class="nav-number">8.5.</span> <span class="nav-text">8.5 DeepSeek-V3 使用 FP8 训练混合精度训练</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-6-DeepSeek-V3-%E7%9A%84-DualPipe-%E7%AE%97%E6%B3%95%E8%A7%A3%E5%86%B3%E9%80%9A%E4%BF%A1%E7%93%B6%E9%A2%88"><span class="nav-number">8.6.</span> <span class="nav-text">8.6 DeepSeek-V3 的 DualPipe 算法解决通信瓶颈</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-7-DeepSeek-V3-%E4%B8%8E%E4%BC%A0%E7%BB%9F%E7%9A%84%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%E7%9A%84%E4%B8%8D%E5%90%8C"><span class="nav-number">8.7.</span> <span class="nav-text">8.7 DeepSeek-V3 与传统的知识蒸馏的不同</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-8-DeepSeek-V3-%E4%BD%BF%E7%94%A8-tile-wise-%E5%92%8C-block-wise-%E9%87%8F%E5%8C%96"><span class="nav-number">8.8.</span> <span class="nav-text">8.8 DeepSeek-V3 使用 tile-wise 和 block-wise 量化</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-9-DeepSeek-R1-Zero-%E7%A0%94%E7%A9%B6%E7%BA%AF%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%EF%BC%88RL%EF%BC%89%E5%9C%A8-LLM-%E6%8E%A8%E7%90%86%E8%83%BD%E5%8A%9B%E4%B8%8A%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">8.9.</span> <span class="nav-text">8.9 DeepSeek-R1-Zero 研究纯强化学习（RL）在 LLM 推理能力上的应用</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-10-DeepSeek-R1-Zero-%E7%9A%84%E3%80%8C%E9%A1%BF%E6%82%9F%E3%80%8D%E6%97%B6%E5%88%BB-aha-moment"><span class="nav-number">8.10.</span> <span class="nav-text">8.10 DeepSeek-R1-Zero 的「顿悟」时刻 aha moment</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-11-DeepSeek-R1%E7%9B%B8%E6%AF%94DeepSeek-R1-Zero-%E5%81%9A%E5%87%BA%E7%9A%84%E6%94%B9%E8%BF%9B"><span class="nav-number">8.11.</span> <span class="nav-text">8.11 DeepSeek-R1相比DeepSeek-R1-Zero 做出的改进</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-12-DeepSeek-R1-%E8%A6%81%E4%BD%BF%E7%94%A8%E5%86%B7%E5%90%AF%E5%8A%A8%E6%95%B0%E6%8D%AE"><span class="nav-number">8.12.</span> <span class="nav-text">8.12 DeepSeek-R1 要使用冷启动数据</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-13-DeepSeek-R1-%E7%9A%84%E5%A4%9A%E9%98%B6%E6%AE%B5%E8%AE%AD%E7%BB%83%E6%A1%86%E6%9E%B6%E4%B8%AD%E6%AF%8F%E4%B8%AA%E9%98%B6%E6%AE%B5%E7%9A%84%E4%BE%A7%E9%87%8D%E7%82%B9"><span class="nav-number">8.13.</span> <span class="nav-text">8.13 DeepSeek-R1 的多阶段训练框架中每个阶段的侧重点</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-14-DeepSeekMath-Corpus-%E7%9A%84%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B%EF%BC%8C%E9%9C%80%E8%A6%81%E8%BF%AD%E4%BB%A3%E5%BC%8F%E5%9C%B0%E6%94%B6%E9%9B%86%E6%95%B0%E6%8D%AE"><span class="nav-number">8.14.</span> <span class="nav-text">8.14 DeepSeekMath Corpus 的构建过程，需要迭代式地收集数据</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-15-DeepSeekMath-Base-%E4%BB%8E-DeepSeek-Coder-Base-%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%A7%8B%E8%BF%9B%E8%A1%8C%E6%95%B0%E5%AD%A6%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-number">8.15.</span> <span class="nav-text">8.15 DeepSeekMath-Base 从 DeepSeek-Coder-Base 模型开始进行数学预训练</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-16-DeepSeekMath-RL-%E9%80%9A%E8%BF%87-GRPO-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%BF%9B%E4%B8%80%E6%AD%A5%E4%BC%98%E5%8C%96%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="nav-number">8.16.</span> <span class="nav-text">8.16 DeepSeekMath-RL 通过 GRPO 强化学习进一步优化的模型</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#8-17-RL-%E7%9A%84%E6%9C%AA%E6%9D%A5%E6%96%B9%E5%90%91"><span class="nav-number">8.17.</span> <span class="nav-text">8.17 RL 的未来方向</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiawei Li"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiawei Li</p>
  <div class="site-description" itemprop="description">愚蠢的zzz侠</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiawei Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
