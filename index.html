<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="愚蠢的zzz侠">
<meta property="og:type" content="website">
<meta property="og:title" content="睿智的ljw侠">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="睿智的ljw侠">
<meta property="og:description" content="愚蠢的zzz侠">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Jiawei Li">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>睿智的ljw侠</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">睿智的ljw侠</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/" class="post-title-link" itemprop="url">17.使用KVM创建虚拟机并配置NPU直通与vNPU直通</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-02-11 15:41:05 / 修改时间：16:13:21" itemprop="dateCreated datePublished" datetime="2025-02-11T15:41:05+08:00">2025-02-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本文介绍如何使用KVM虚拟化技术在Ubuntu 22.04系统中创建虚拟机，并完成NPU直通与vNPU直通配置，适用于多台虚拟机共享同一物理NPU资源。</p>
<hr>
<h5 id="1-参考"><a href="#1-参考" class="headerlink" title="1. 参考"></a>1. 参考</h5><ul>
<li><p><a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100288557/52841d71">Atlas 中心训练服务器 6.0.0 NPU驱动和固件安装指南 06-宿主机目录不挂载到容器</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100388928?idPath=23710424%7C251366513%7C254884019%7C261408772%7C252764743">Atlas 系列硬件产品 24.1.RC2 虚拟机配置指南 05</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100332931/d140d2ee?idPath=23710424%7C251366513%7C22892968%7C252309113%7C254184887">虚拟机安装Ubuntu 20.04&#x2F;Ubuntu 22.04</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100332931/fa6d812d?idPath=23710424%7C251366513%7C22892968%7C252309113%7C254184887">vNPU直通虚拟机</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/quick-installation/23.0.RC3/quickinstg_train/800_9000A2/quickinstg_800_9000A2_0043.html">虚拟机安装NPU驱动-虚拟机部署场景-昇腾软件安装指南-Atlas 800T A2 训练服务器-训练部署指南-快速部署7.0.RC1开发文档-昇腾社区</a></p>
</li>
</ul>
<hr>
<h5 id="2-创建虚拟机磁盘空间"><a href="#2-创建虚拟机磁盘空间" class="headerlink" title="2. 创建虚拟机磁盘空间"></a>2. 创建虚拟机磁盘空间</h5><h6 id="2-1-开启网络桥接"><a href="#2-1-开启网络桥接" class="headerlink" title="2.1 开启网络桥接"></a>2.1 开启网络桥接</h6><p>首先，启用虚拟机的网络桥接模式，确保虚拟机能够访问网络。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">virsh net-list --all</span><br><span class="line">virsh net-start default</span><br><span class="line"><span class="comment"># virsh net-autostart default</span></span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-17-08-09-image.png"></p>
<hr>
<h6 id="2-2-创建存储池"><a href="#2-2-创建存储池" class="headerlink" title="2.2 创建存储池"></a>2.2 创建存储池</h6><p>在虚拟机的宿主机上创建存储池并设置访问权限。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /home/kvm/images</span><br><span class="line"><span class="built_in">chown</span> root:root /home/kvm/images</span><br><span class="line"><span class="built_in">chmod</span> 755 /home/kvm/images</span><br><span class="line">yum install libvirt</span><br><span class="line">systemctl status libvirtd</span><br><span class="line">systemctl start libvirtd</span><br><span class="line">virsh pool-define-as StoragePool --<span class="built_in">type</span> <span class="built_in">dir</span> --target /home/kvm/images</span><br><span class="line">virsh pool-build StoragePool</span><br><span class="line">virsh pool-start StoragePool</span><br><span class="line">virsh pool-autostart StoragePool</span><br><span class="line">virsh pool-list</span><br><span class="line">virsh pool-info StoragePool</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-09-17-46-06-image.png"></p>
<hr>
<h6 id="2-3-创建虚拟机磁盘空间"><a href="#2-3-创建虚拟机磁盘空间" class="headerlink" title="2.3 创建虚拟机磁盘空间"></a>2.3 创建虚拟机磁盘空间</h6><p>创建虚拟机磁盘文件并指定格式和容量。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">virsh vol-create-as --pool StoragePool --name ubuntu.img --capacity 50G --allocation 1G --format qcow2</span><br><span class="line">virsh vol-info /home/kvm/images/ubuntu.img</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-09-17-48-52-image.png"></p>
<hr>
<h6 id="2-4-创建虚拟机"><a href="#2-4-创建虚拟机" class="headerlink" title="2.4 创建虚拟机"></a>2.4 创建虚拟机</h6><p>下载Ubuntu 22.04的ISO镜像，并开始创建虚拟机。确保安装好X服务器以支持图形界面的X11转发。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># getenforce # 检查selinux状态，显示Disabled</span></span><br><span class="line">wget https://cdimage.ubuntu.com/releases/22.04/release/ubuntu-22.04.5-live-server-arm64.iso </span><br><span class="line"></span><br><span class="line">yum install virt-install virt-viewer</span><br><span class="line">virt-install --name=ubuntu --vcpus=4 --ram=8192 --disk path=/home/kvm/images/ubuntu.img,format=qcow2,size=50,bus=virtio --machine virt-4.1 --cdrom /home/ljw/pkgs/ubuntu-22.04.5-live-server-arm64.iso --network bridge=virbr0,model=virtio --force --graphic vnc,listen=0.0.0.0,port=5906 --input <span class="built_in">type</span>=tablet,bus=usb --input <span class="built_in">type</span>=keyboard,bus=virtio</span><br><span class="line"></span><br><span class="line"><span class="comment"># virsh list --all  # 确认虚拟机状态</span></span><br><span class="line"><span class="comment"># virsh undefine --nvram ubuntu  # 删除 NVRAM 配置</span></span><br><span class="line"><span class="comment"># virsh shutdown ubuntu  # 优雅地关闭虚拟机</span></span><br><span class="line"><span class="comment"># virsh destroy ubuntu   # 直接终止虚拟机</span></span><br><span class="line"><span class="comment"># virsh start ubuntu  # 启动</span></span><br><span class="line"><span class="comment"># virt-viewer vnc://127.0.0.1:5906  # 重新启动 Virt-Viewer</span></span><br></pre></td></tr></table></figure>

<hr>
<blockquote>
<p>查询当前环境支持的机器类型（可选）</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">pip install sphinx</span><br><span class="line">yum install ninja-build</span><br><span class="line">pip install sphinx_rtd_theme</span><br><span class="line">git <span class="built_in">clone</span> https://gitlab.com/qemu-project/qemu.git</span><br><span class="line"><span class="built_in">cd</span> qemu</span><br><span class="line">yum install zlib-devel libaio-devel libcurl-devel</span><br><span class="line"></span><br><span class="line">wget http://ftp.gnu.org/gnu/gcc/gcc-7.4.0/gcc-7.4.0.tar.gz</span><br><span class="line">yum install mpfr-devel gmp-devel libmpc-devel</span><br><span class="line">tar -xvf gcc-7.4.0.tar.gz</span><br><span class="line"><span class="built_in">cd</span> gcc-7.4.0</span><br><span class="line">./configure --disable-multilib --enable-languages=c,c++ --prefix=/usr/local</span><br><span class="line">make -j$(<span class="built_in">nproc</span>)</span><br><span class="line">make install</span><br><span class="line"><span class="built_in">ln</span> -sf /usr/local/bin/gcc /usr/bin/gcc</span><br><span class="line"></span><br><span class="line"><span class="built_in">cd</span> ../qemu</span><br><span class="line">./configure --target-list=aarch64-softmmu</span><br><span class="line">make -j$(<span class="built_in">nproc</span>)</span><br><span class="line">make install</span><br><span class="line">qemu-system-aarch64 --version</span><br><span class="line">qemu-system-aarch64 -machine <span class="built_in">help</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p>在个人电脑上安装<a target="_blank" rel="noopener" href="https://sourceforge.net/projects/xming/">X 服务器</a>，且在ssh连接时选择X11转发图形界面</p>
</blockquote>
<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-13-32-37-image.png"></p>
<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-13-33-40-image.png"></p>
<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-17-23-10-image.png"></p>
<hr>
<h6 id="2-5-从宿主机连接虚拟机"><a href="#2-5-从宿主机连接虚拟机" class="headerlink" title="2.5 从宿主机连接虚拟机"></a>2.5 从宿主机连接虚拟机</h6><p>获取虚拟机IP地址，使用SSH连接至虚拟机：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip a</span><br><span class="line">ssh x@192.168.122.244</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-13-58-28-image.png"></p>
<hr>
<h5 id="3-NPU直通虚拟机（不用这个）"><a href="#3-NPU直通虚拟机（不用这个）" class="headerlink" title="3. NPU直通虚拟机（不用这个）"></a>3. NPU直通虚拟机（不用这个）</h5><p>NPU直通可以让虚拟机直接访问物理NPU设备，提升计算性能。</p>
<p>查询NPU芯片的PCIe信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lspci | grep d802  <span class="comment"># C1:00.0 Processing accelerators: Huawei Technologies Co., Ltd. Device d802 (rev 20)</span></span><br></pre></td></tr></table></figure>

<p>获取NPU设备的详细信息：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">virsh nodedev-list --tree | grep pci  <span class="comment"># pci_0000_c1_00_0</span></span><br><span class="line">virsh nodedev-dumpxml pci_0000_c1_00_0</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-15-06-19-image.png"></p>
<p>关闭虚拟机并编辑虚拟机配置，将NPU直通至虚拟机：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">virsh shutdown ubuntu</span><br><span class="line">virsh edit ubuntu</span><br></pre></td></tr></table></figure>

<p>编辑配置文件，将NPU设备配置为<code>hostdev</code>模式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;hostdev mode=<span class="string">&#x27;subsystem&#x27;</span> <span class="built_in">type</span>=<span class="string">&#x27;pci&#x27;</span> managed=<span class="string">&#x27;yes&#x27;</span>&gt;</span><br><span class="line">  &lt;<span class="built_in">source</span>&gt;</span><br><span class="line">    &lt;address domain=<span class="string">&#x27;0&#x27;</span> bus=<span class="string">&#x27;193&#x27;</span> slot=<span class="string">&#x27;0&#x27;</span> <span class="keyword">function</span>=<span class="string">&#x27;0&#x27;</span>/&gt;</span><br><span class="line">  &lt;/source&gt;</span><br><span class="line"> &lt;/hostdev&gt;</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-15-27-44-image.png"></p>
<p>启动虚拟机：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virsh start ubuntu</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="4-vNPU直通虚拟机"><a href="#4-vNPU直通虚拟机" class="headerlink" title="4. vNPU直通虚拟机"></a>4. vNPU直通虚拟机</h5><p>vNPU是虚拟化NPU资源，使多台虚拟机可以共享同一物理NPU的计算能力。以下是如何配置vNPU的步骤：</p>
<p>设置vNPU模式：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npu-smi <span class="built_in">set</span> -t vnpu-mode -d 0</span><br></pre></td></tr></table></figure>

<p>列出NPU支持的算力切分模板：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npu-smi info -t template-info -i 0  <span class="comment"># 0 # vir10_3c_32g</span></span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-16-56-24-image.png"></p>
<p>切分NPU资源并创建虚拟NPU设备：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">find / -name sriov_numvfs  <span class="comment"># /sys/devices/pci0000:c0/0000:c0:00.0/0000:c1:00.0/sriov_numvfs</span></span><br><span class="line"><span class="built_in">cd</span> /sys/devices/pci0000:c0/0000:c0:00.0/0000:c1:00.0/</span><br><span class="line"><span class="built_in">echo</span> 8 &gt; sriov_numvfs</span><br><span class="line">npu-smi info  <span class="comment"># 0000:C1:00.0</span></span><br><span class="line"><span class="built_in">cat</span> /proc/sys/kernel/random/uuid  <span class="comment"># 8767b5f0-abce-4fa8-a112-8c928feb29d7</span></span><br><span class="line"><span class="built_in">echo</span> 8767b5f0-abce-4fa8-a112-8c928feb29d7 &gt; /sys/bus/pci/devices/0000\:c1\:00.0/mdev_supported_types/vnpu-vir10_3c_32g/create  </span><br><span class="line"><span class="built_in">ls</span> -l /sys/bus/mdev/devices/</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-17-35-40-image.png"></p>
<p>关闭虚拟机并编辑虚拟机配置，将vNPU直通虚拟机：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">virsh list --all  <span class="comment"># ubuntu</span></span><br><span class="line">virsh shutdown ubuntu</span><br><span class="line">virsh edit ubuntu</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">hostdev</span> <span class="attr">mode</span>=<span class="string">&#x27;subsystem&#x27;</span> <span class="attr">type</span>=<span class="string">&#x27;mdev&#x27;</span> <span class="attr">model</span>=<span class="string">&#x27;vfio-pci&#x27;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">source</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">address</span> <span class="attr">uuid</span>=<span class="string">&#x27;8767b5f0-abce-4fa8-a112-8c928feb29d7&#x27;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">hostdev</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-17-40-01-image.png"></p>
<p>重新打开之后会自动更新格式。启动虚拟机并确认vNPU配置成功：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">virsh start ubuntu</span><br><span class="line">ssh x@192.168.122.244</span><br><span class="line">lspci | grep d802</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-17-42-06-image.png"></p>
<hr>
<h5 id="5-虚拟机安装NPU驱动"><a href="#5-虚拟机安装NPU驱动" class="headerlink" title="5. 虚拟机安装NPU驱动"></a>5. 虚拟机安装NPU驱动</h5><p>将驱动文件上传到虚拟机并完成安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp Ascend-hdk-910b-npu-driver_24.1.rc2_linux-aarch64.run x@192.168.122.244:/home/ljw/pkgs</span><br></pre></td></tr></table></figure>

<p>安装NPU驱动：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt update</span><br><span class="line"><span class="built_in">sudo</span> apt install build-essential</span><br><span class="line">./Ascend-hdk-910b-npu-driver_24.1.rc2_linux-aarch64.run --full --install-for-all</span><br><span class="line"><span class="comment"># ./Ascend-hdk-910b-npu-driver_24.1.rc2_linux-aarch64.run --debug # 不支持</span></span><br><span class="line"><span class="comment"># /usr/local/Ascend/driver/script/uninstall.sh  # 卸载</span></span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-18-06-26-image.png"></p>
<hr>
<h5 id="6-切换虚拟机内核"><a href="#6-切换虚拟机内核" class="headerlink" title="6. 切换虚拟机内核"></a>6. 切换虚拟机内核</h5><p>有时虚拟机需要使用特定版本的内核。以下是如何在虚拟机中切换内核版本，虚拟机自身内核是5.15.0-131-generic，切换成5.15.0-25-generic</p>
<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-18-59-33-image.png"></p>
<p>安装所需内核版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">dpkg --list | grep linux-image</span><br><span class="line"><span class="built_in">sudo</span> apt update</span><br><span class="line"><span class="built_in">sudo</span> apt install linux-image-5.15.0-25-generic</span><br><span class="line"><span class="built_in">sudo</span> apt-cache search linux-image-5.15.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>更新GRUB配置文件。将 GRUB_DEFAULT&#x3D;0 修改为：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> nano /etc/default/grub</span><br><span class="line">GRUB_DEFAULT=<span class="string">&quot;Advanced options for Ubuntu&gt;Ubuntu, with Linux 5.15.0-25-generic&quot;</span></span><br></pre></td></tr></table></figure>

<p>更新GRUB并重启：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> update-grub</span><br><span class="line">reboot</span><br><span class="line"><span class="built_in">uname</span> -a</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/11/17.%E4%BD%BF%E7%94%A8KVM%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%B9%B6%E9%85%8D%E7%BD%AENPU%E7%9B%B4%E9%80%9A%E4%B8%8EvNPU%E7%9B%B4%E9%80%9A/2025-02-10-18-58-17-image.png"></p>
<p>删除不需要的内核版本：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dpkg --list | grep linux-image</span><br><span class="line"><span class="built_in">sudo</span> apt remove --purge linux-image-5.15.0-131-generic linux-headers-5.15.0-131</span><br><span class="line"><span class="built_in">sudo</span> apt remove --purge linux-modules-5.15.0-131-generic</span><br><span class="line"><span class="built_in">sudo</span> apt autoremove</span><br><span class="line"><span class="built_in">sudo</span> update-grub</span><br><span class="line"><span class="built_in">sudo</span> reboot</span><br></pre></td></tr></table></figure>

<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/08/16.%E4%BD%BF%E7%94%A8msDebug%E8%BF%9B%E8%A1%8C%E7%AE%97%E5%AD%90%E8%B0%83%E8%AF%95--%E6%9C%AA%E5%AE%8C%E6%88%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/08/16.%E4%BD%BF%E7%94%A8msDebug%E8%BF%9B%E8%A1%8C%E7%AE%97%E5%AD%90%E8%B0%83%E8%AF%95--%E6%9C%AA%E5%AE%8C%E6%88%90/" class="post-title-link" itemprop="url">16.使用msDebug进行算子调试--未完成</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-08 17:41:05" itemprop="dateCreated datePublished" datetime="2025-02-08T17:41:05+08:00">2025-02-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-11 16:13:52" itemprop="dateModified" datetime="2025-02-11T16:13:52+08:00">2025-02-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5 id="1-参考"><a href="#1-参考" class="headerlink" title="1. 参考"></a>1. 参考</h5><ul>
<li><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha003/devaids/opdev/optool/atlasopdev_16_0064.html">工具启动-算子调试（msDebug）-算子开发工具-算子开发-CANN社区版8.0.0.alpha003开发文档-昇腾社区</a></li>
</ul>
<hr>
<h5 id="2-查看-o文件所在位置"><a href="#2-查看-o文件所在位置" class="headerlink" title="2. 查看.o文件所在位置"></a>2. 查看.o文件所在位置</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ls</span> /home/ljw/ops/GridSampler2dTest/910B3/GridSampler2dTest/build_out/kernel/ascend910b/grid_sampler2d_test</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/08/16.%E4%BD%BF%E7%94%A8msDebug%E8%BF%9B%E8%A1%8C%E7%AE%97%E5%AD%90%E8%B0%83%E8%AF%95--%E6%9C%AA%E5%AE%8C%E6%88%90/2025-02-08-16-40-39-image.png"></p>
<hr>
<h5 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3. 配置环境变量"></a>3. 配置环境变量</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> TERMINFO=/usr/share/terminfo</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> LAUNCH_KERNEL_PATH=/home/ljw/ops/GridSampler2dTest/910B3/GridSampler2dTest/build_out/kernel/ascend910b/grid_sampler2d_test/GridSampler2dTest_07816956234200e69338fc47875aa709.o   </span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/local/Ascend/ascend-toolkit/latest/opp/vendors/customize/op_api/lib:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure>

<hr>
<h5 id="4-使用msDebug进行调试"><a href="#4-使用msDebug进行调试" class="headerlink" title="4. 使用msDebug进行调试"></a>4. 使用msDebug进行调试</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用msDebug工具加载可执行文件</span></span><br><span class="line">msdebug execute_grid_sampler2d_test_op </span><br><span class="line"><span class="comment"># 增加断点</span></span><br><span class="line">b /home/ljw/ops/GridSampler2dTest/910B3/GridSampler2dTest/op_kernel/grid_sampler2d_test.cpp:677</span><br><span class="line"><span class="comment"># 运行算子程序，等待直到命中断点</span></span><br><span class="line">run</span><br><span class="line"><span class="comment"># 显示断点</span></span><br><span class="line">breakpoint list </span><br><span class="line"><span class="comment"># 删除断点</span></span><br><span class="line">breakpoint delete 1</span><br><span class="line"><span class="comment"># 恢复程序运行</span></span><br><span class="line">c</span><br><span class="line"><span class="comment"># 退出</span></span><br><span class="line">q</span><br></pre></td></tr></table></figure>

<p><img src="/2025/02/08/16.%E4%BD%BF%E7%94%A8msDebug%E8%BF%9B%E8%A1%8C%E7%AE%97%E5%AD%90%E8%B0%83%E8%AF%95--%E6%9C%AA%E5%AE%8C%E6%88%90/2025-02-08-17-13-07-image.png"></p>
<hr>
<h5 id="5-待完成"><a href="#5-待完成" class="headerlink" title="5. 待完成"></a>5. 待完成</h5><p><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha003/devaids/opdev/optool/atlasopdev_16_0080.html#ZH-CN_TOPIC_0000002151419829__zh-cn_topic_0000001850780656_li064693084719">msDebug工具在容器环境中调试运行失败，提示需安装HDK驱动包-FAQ-算子调试（msDebug）-算子开发工具-算子开发-CANN社区版8.0.0.alpha003开发文档-昇腾社区</a></p>
<p>需要重新安装驱动且指定 –debug 参数，暂时没机会重新安装，（msKPP算子设计、msDebug算子调试）功能待之后使用。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./Ascend-hdk-&lt;chip_type&gt;-npu-driver_&lt;version&gt;_linux-&lt;<span class="built_in">arch</span>&gt;.run --debug</span><br></pre></td></tr></table></figure>

<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/08/15-%E4%BD%BF%E7%94%A8Python%E8%AF%BB%E5%8F%96%E5%B9%B6%E6%89%93%E5%8D%B0bin%E6%96%87%E4%BB%B6%E6%B5%AE%E7%82%B9%E6%95%B0%E6%8D%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/08/15-%E4%BD%BF%E7%94%A8Python%E8%AF%BB%E5%8F%96%E5%B9%B6%E6%89%93%E5%8D%B0bin%E6%96%87%E4%BB%B6%E6%B5%AE%E7%82%B9%E6%95%B0%E6%8D%AE/" class="post-title-link" itemprop="url">15-使用Python读取并打印bin文件浮点数据</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-02-08 11:49:05 / 修改时间：11:43:59" itemprop="dateCreated datePublished" datetime="2025-02-08T11:49:05+08:00">2025-02-08</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>一个简单的 Python 脚本示例，展示了如何从文件加载 <code>float32</code> 数据，然后根据给定的形状进行重塑（reshape）并打印。</p>
<hr>
<h5 id="1-代码示例"><a href="#1-代码示例" class="headerlink" title="1. 代码示例"></a>1. 代码示例</h5><p>使用了 <code>numpy</code> 库的 <code>fromfile()</code> 函数来读取二进制文件中的数据，并指定其类型为 <code>float32</code>。由于默认读取得到的是一维数组（<code>shape</code> 为 <code>(n,)</code>），脚本会将其 <code>reshape</code> 到指定的输出形状。最后，脚本将重塑之后的结果打印出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_and_print_golden</span>(<span class="params">path=<span class="string">&quot;../output/golden.bin&quot;</span>, output_shape=(<span class="params"><span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span></span>)</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    从指定路径读取二进制文件，并将其转为 float32 的 numpy 数组。</span></span><br><span class="line"><span class="string">    然后根据 output_shape 对数据进行 reshape 并打印。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param path: 二进制文件路径，默认为 &quot;../output/golden.bin&quot;</span></span><br><span class="line"><span class="string">    :param output_shape: 需要 reshape 到的目标形状，默认为 (1, 3, 2, 2)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 1) 从文件读取所有 float32 类型的数据</span></span><br><span class="line">    data = np.fromfile(path, dtype=np.float32)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Read raw data:&quot;</span>, data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Data shape before reshape:&quot;</span>, data.shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2) 将一维数组 reshape 成指定的形状</span></span><br><span class="line">    data_reshaped = data.reshape(output_shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Reshaped data:\n&quot;</span>, data_reshaped)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Reshaped data shape:&quot;</span>, data_reshaped.shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 获取命令行传入的参数</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sys.argv) &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Usage: python printData.py &lt;path&gt; &lt;output_shape&gt;&quot;</span>)</span><br><span class="line">        sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从命令行参数中读取路径和形状</span></span><br><span class="line">    path = sys.argv[<span class="number">1</span>]  <span class="comment"># 第一个参数是文件路径</span></span><br><span class="line">    output_shape = <span class="built_in">tuple</span>(<span class="built_in">map</span>(<span class="built_in">int</span>, sys.argv[<span class="number">2</span>].strip(<span class="string">&#x27;()&#x27;</span>).split(<span class="string">&#x27;,&#x27;</span>)))  <span class="comment"># 第二个参数是目标形状，如 (1, 3, 2, 2)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 调用函数</span></span><br><span class="line">    load_and_print_golden(path, output_shape)</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="2-运行方式"><a href="#2-运行方式" class="headerlink" title="2. 运行方式"></a>2. 运行方式</h5><p>运行脚本并传入参数</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python3 printData.py</span><br><span class="line">python3 printData.py <span class="string">&quot;../output/output.bin&quot;</span> 12</span><br><span class="line">python3 printData.py <span class="string">&quot;../output/golden.bin&quot;</span> <span class="string">&quot;(1, 3, 2, 2)&quot;</span></span><br></pre></td></tr></table></figure>

<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/05/14-Git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/05/14-Git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/" class="post-title-link" itemprop="url">14-Git基本操作流程及注意事项</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-05 14:49:05" itemprop="dateCreated datePublished" datetime="2025-02-05T14:49:05+08:00">2025-02-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-07 17:06:07" itemprop="dateModified" datetime="2025-02-07T17:06:07+08:00">2025-02-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5 id="1-克隆仓库"><a href="#1-克隆仓库" class="headerlink" title="1. 克隆仓库"></a>1. 克隆仓库</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/ljw89757/ops.git</span><br></pre></td></tr></table></figure>

<h5 id="2-查看当前的状态"><a href="#2-查看当前的状态" class="headerlink" title="2. 查看当前的状态"></a>2. 查看当前的状态</h5><p>在对文件做了修改后，可以随时查看工作区和暂存区的状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure>

<h5 id="3-将新的内容添加到暂存区"><a href="#3-将新的内容添加到暂存区" class="headerlink" title="3. 将新的内容添加到暂存区"></a>3. 将新的内容添加到暂存区</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></table></figure>

<p><code>.</code> 表示将当前目录下所有有变动的文件添加到暂存区</p>
<h5 id="4-提交更改"><a href="#4-提交更改" class="headerlink" title="4. 提交更改"></a>4. 提交更改</h5><p>在完成对代码的编辑并将文件添加到暂存区后，需要进行提交（Commit），并为提交添加一条注释信息。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m <span class="string">&quot;Add new op GirdSample2D&quot;</span></span><br></pre></td></tr></table></figure>

<h5 id="5-推送到远程仓库"><a href="#5-推送到远程仓库" class="headerlink" title="5. 推送到远程仓库"></a>5. 推送到远程仓库</h5><p>如果已有远程仓库并希望将本地提交同步过去，需要先处理好分支，然后执行 <code>git push</code>。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看本地分支列表</span></span><br><span class="line">git branch </span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建并切换到 main 分支（假设远程仓库使用 main 作为主分支）</span></span><br><span class="line">git checkout -b main  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果本地仓库和远程仓库在历史上从未关联过，可以先拉取远程仓库的 main 分支</span></span><br><span class="line">git pull origin main --allow-unrelated-histories</span><br><span class="line"><span class="comment"># git reset --hard # 放弃本地修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后再将本地代码推送到远程，并设置为跟踪分支</span></span><br><span class="line">git push -u origin main</span><br><span class="line"></span><br><span class="line"><span class="comment"># git push origin main --force</span></span><br></pre></td></tr></table></figure>

<hr>
<h5 id="6-将远程仓库与本地仓库关联"><a href="#6-将远程仓库与本地仓库关联" class="headerlink" title="6. 将远程仓库与本地仓库关联"></a>6. 将远程仓库与本地仓库关联</h5><p>如果在一开始是从零创建的本地项目，需要先给仓库添加一个远程地址，以便后续进行推送。如：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br><span class="line">git remote add origin https://github.com/ljw89757/ops.git</span><br></pre></td></tr></table></figure>

<p>执行此命令后，<code>origin</code> 就代表了远程仓库的地址。</p>
<h5 id="7-检查当前的远程仓库设置"><a href="#7-检查当前的远程仓库设置" class="headerlink" title="7. 检查当前的远程仓库设置"></a>7. 检查当前的远程仓库设置</h5><p>列出所有配置过的远程仓库的名称和对应地址</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote -v</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="8-移除历史中的大文件"><a href="#8-移除历史中的大文件" class="headerlink" title="8. 移除历史中的大文件"></a>8. 移除历史中的大文件</h5><p><img src="/2025/02/05/14-Git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/2025-02-05-13-53-48-image.png"></p>
<p>即使已经在本地已经删除了这些文件，它们仍然可能存在于 Git 的历史记录中，因此当你尝试推送到远程仓库时，GitHub 会识别出这些大文件并抛出错误。可以使用 <code>git filter-branch</code> 来从 Git 历史中删除指定的文件。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git filter-branch --force --index-filter \</span><br><span class="line">  <span class="string">&quot;git rm --cached --ignore-unmatch ops/GroupNormV2Sample/FrameworkLaunch/AclNNInvocation/GroupNormV2Case1/output/OPPROF_20241014030459_LBUSAYWGDMBXWHGR/simulator/visualize_data.bin&quot;</span> \</span><br><span class="line">  --prune-empty --tag-name-filter <span class="built_in">cat</span> -- --all</span><br></pre></td></tr></table></figure>

<p>清理剩余的 Git 历史垃圾</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reflog expire --expire=now --all</span><br><span class="line">git gc --prune=now</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="9-启用长路径支持"><a href="#9-启用长路径支持" class="headerlink" title="9. 启用长路径支持"></a>9. 启用长路径支持</h5><p><img src="/2025/02/05/14-Git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/2025-02-05-11-44-57-image.png"></p>
<p>这个错误 <code>Filename too long</code> 表示 Git 在克隆仓库时遇到文件路径过长的问题。Windows 系统有一个路径长度限制，默认最大路径为 260 个字符（包括文件夹和文件名）。因此，当文件或文件夹的路径超出这个限制时，就会出现这样的错误。</p>
<p><strong>方法一：启用长路径支持（注册表修改）</strong></p>
<p>使用注册表启用。按下 <code>Win + R</code>，然后输入 <code>regedit</code> 打开注册表编辑器。</p>
<p>导航到以下路径：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\FileSystem</span><br></pre></td></tr></table></figure>

<p>找到名为 <strong>LongPathsEnabled</strong> 的键，设置该键的值为 <code>1</code>，然后重启计算机。</p>
<p><img src="/2025/02/05/14-Git%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/2025-02-05-11-47-30-image.png"></p>
<p><strong>方法二：配置 Git 启用长路径支持</strong></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global core.longpaths <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<hr>
<h5 id="10-重置指针"><a href="#10-重置指针" class="headerlink" title="10. 重置指针"></a>10. 重置指针</h5><p>用于尚未把提交推送到远程仓库，想要直接把本地代码和分支指针都回退到上一次提交的状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看历史提交</span></span><br><span class="line">git <span class="built_in">log</span> --oneline</span><br><span class="line"><span class="comment"># 重置到上一个提交</span></span><br><span class="line">git reset --hard HEAD^</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="11-总结"><a href="#11-总结" class="headerlink" title="11. 总结"></a>11. 总结</h5><ol>
<li>初始化或克隆仓库：本地要么用 <code>git clone</code> 从远程获取项目，要么用 <code>git init</code> + <code>git remote add</code> 自行创建并与远程仓库关联。</li>
<li>日常开发：开发中频繁使用 <code>git status</code> 来查看状态，用 <code>git add</code> 暂存文件，用 <code>git commit</code> 提交改动。</li>
<li>分支管理：切换或创建分支，最后通过 <code>git merge</code> 或者 <code>git pull</code> 同步远程分支的改动。</li>
<li>推送与拉取：使用 <code>git push</code> 推送本地分支到远程，使用 <code>git pull</code> 获取远程更新并合并到本地分支。</li>
<li>远程仓库关联：确认远程地址(<code>origin</code>)是否正确，并在需要时可以修改或添加新的远程地址。</li>
</ol>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">13-DeepSeek总结</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-02-03 11:49:05" itemprop="dateCreated datePublished" datetime="2025-02-03T11:49:05+08:00">2025-02-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-05 10:21:30" itemprop="dateModified" datetime="2025-02-05T10:21:30+08:00">2025-02-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h5 id="1-参考"><a href="#1-参考" class="headerlink" title="1. 参考"></a>1. 参考</h5><ul>
<li><p>DeepSeek-V3技术报告：<a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">DeepSeek-V3&#x2F;DeepSeek_V3.pdf at main · deepseek-ai&#x2F;DeepSeek-V3 · GitHub</a></p>
</li>
<li><p>DeepSeek-R1技术报告：<a target="_blank" rel="noopener" href="https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf">DeepSeek-R1&#x2F;DeepSeek_R1.pdf at main · deepseek-ai&#x2F;DeepSeek-R1 · GitHub</a></p>
</li>
<li><p>DeepSeekMath：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2402.03300">[2402.03300] DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models</a></p>
</li>
<li><p>权重（大小足有671B，FP8精度）：<a target="_blank" rel="noopener" href="https://huggingface.co/deepseek-ai/DeepSeek-V3-Base">deepseek-ai&#x2F;DeepSeek-V3-Base · Hugging Face</a></p>
</li>
<li><p>【论文解读】DeepSeek-V3技术报告 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/14988009150">https://zhuanlan.zhihu.com/p/14988009150</a></p>
</li>
<li><p>【论文解读】DeepSeek-R1：通过强化学习提升LLM推理能力 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/19551355661">https://zhuanlan.zhihu.com/p/19551355661</a></p>
</li>
<li><p>【论文解读】DeepSeekMath：用GRPO改进PPO <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/14574329458">https://zhuanlan.zhihu.com/p/14574329458</a></p>
</li>
<li><p>八个问题，带你零基础了解DeepSeek <a target="_blank" rel="noopener" href="https://news.qq.com/rain/a/20250202A05WKP00">https://news.qq.com/rain/a/20250202A05WKP00</a></p>
</li>
</ul>
<hr>
<h5 id="2-背景"><a href="#2-背景" class="headerlink" title="2. 背景"></a>2. 背景</h5><p><img src="https://inews.gtimg.com/news_bt/Ov9p60kvt5U-wTjM_moXxGc6mmckdZzqW7VPC58iC7XxcAA/641" alt="图片"></p>
<p>DeepSeek已经发布13个大模型，并且都已开源。</p>
<p><img src="https://inews.gtimg.com/news_bt/OUv3cR4Dkjq6H4oKKtczoIWQcAJDzwN6ZoilBtonqCRbcAA/641"></p>
<p>最近引起全世界广泛关注的模型，主要是自研通用大模型 DeepSeek-V3、推理模型 DeepSeek-R1 。（DeepSeek-V3 对标GPT-4o，DeepSeek-R1对标o1）。</p>
<ul>
<li><p>DeepSeek-V3 是一个通用模型，日常常见的问题，都可以尝试使用 V3。</p>
</li>
<li><p>DeepSeek-R1 是一个推理模型，擅长处理复杂、需要多步思考的问题，适合做深度研究、解决代码问题、数学问题。</p>
</li>
</ul>
<p><img src="https://inews.gtimg.com/news_bt/O4s96pmSE_kkUf1hUEIGG1-brd1j22JL7JaFKRTR84aeQAA/641"></p>
<p>目前Web和APP均免费。</p>
<ul>
<li><p>Web 端直接通过访问<a target="_blank" rel="noopener" href="https://chat.deepseek.com/">网址</a>对话。在对话框的左下角位置，可以选择是否开启“深度思考”模式。如果勾选，会使用 DeepSeek-R1 模型；如不勾选，则默认使用 DeepSeek-V3 。</p>
</li>
<li><p>App 直接在应用商店中搜索“DeepSeek”即可。在APP端，可以选择同时使用联网和推理功能。</p>
</li>
</ul>
<p>也可以通过多种渠道调用 DeepSeek 的API:</p>
<ol>
<li><p>DeepSeek开发者平台：访问 DeepSeek 控制台</p>
<p><a target="_blank" rel="noopener" href="https://platform.deepseek.com/%EF%BC%8C%E6%B3%A8%E5%86%8C%E7%99%BB%E5%BD%95%E5%B9%B6%E8%B4%AD%E4%B9%B0%E8%8E%B7%E5%8F%96%E7%9B%B8%E5%BA%94%E7%9A%84%E5%AF%86%E9%92%A5%E3%80%82%EF%BC%88%E4%B8%8D%E8%BF%87%EF%BC%8C%E8%BF%91%E6%9C%9F%E8%AF%A5%E5%B9%B3%E5%8F%B0%E6%AD%A3%E5%9C%A8%E7%BB%B4%E6%8A%A4%E5%BD%93%E4%B8%AD%EF%BC%89">https://platform.deepseek.com/，注册登录并购买获取相应的密钥。（不过，近期该平台正在维护当中）</a></p>
</li>
<li><p>英伟达 NIM 微服务：</p>
<p><a target="_blank" rel="noopener" href="https://build.nvidia.com/deepseek-ai/deepseek-r1%EF%BC%8C%E6%94%AF%E6%8C%81API%E8%B0%83%E7%94%A8">https://build.nvidia.com/deepseek-ai/deepseek-r1，支持API调用</a> DeepSeek-R1，需要使用邮箱注册账号。</p>
</li>
<li><p>微软 Azure：</p>
<p><a target="_blank" rel="noopener" href="https://ai.azure.com,微软/">https://ai.azure.com，微软</a> Azure 可以通过聊天操场，部署DeepSeek-R1，创建一个聊天机器人。</p>
</li>
<li><p>亚马逊 AWS：</p>
<p><a target="_blank" rel="noopener" href="https://aws.amazon.com/cn/blogs/aws/deepseek-r1-models-now-available-on-aws%EF%BC%8CDeepSeek-R1">https://aws.amazon.com/cn/blogs/aws/deepseek-r1-models-now-available-on-aws，DeepSeek-R1</a> 现已在 Amazon Bedrock Marketplace 和 Amazon SageMaker JumpStart 中推出，还可以在 Amazon Bedrock Custom Model Import 和 Amazon EC2 实例来使用 DeepSeek-R1-Distill 模型。</p>
</li>
<li><p>硅基流动 SiliconCloud ：</p>
<p><a target="_blank" rel="noopener" href="https://siliconflow.cn/zh-cn/models">https://siliconflow.cn/zh-cn/models</a> ，上线了基于华为云昇腾云服务的 DeepSeek-V3、DeepSeek-R1，开发者可以直接调用 SiliconCloud API，价格与 DeepSeek 官方优惠期价格保持一致。</p>
</li>
<li><p>此外，Cerebras、Groq 也可以调用 DeepSeek-R1 的 API。</p>
</li>
</ol>
<hr>
<h5 id="3-DeepSeek的优势"><a href="#3-DeepSeek的优势" class="headerlink" title="3. DeepSeek的优势"></a>3. DeepSeek的优势</h5><p>特点：</p>
<ul>
<li><p>性能优秀：这两款模型的性能接近甚至在某些场景超越了“公认”的全球标杆公司OpenAI的最好产品</p>
</li>
<li><p>结合应用：模型发布后，上线DeepSeek的Web&#x2F;APP，可切身体验模型效果。</p>
</li>
<li><p>训练成本低，产品性价比高</p>
<ul>
<li><p>根据 DeepSeek 的官方技术报告，V3 的训练成本仅 557.6 万美元。OpenAI 虽然没有官方公布过 4o 的训练成本，但据OpenAI CEO Sam Altman 透露，GPT-4 的训练总计花费了约1亿美元。</p>
</li>
<li><p>V3 仅使用了 2048 个 H800 GPU、花费2个月训练完成，使用GPU的数量和训练时长颠覆传统认知。</p>
</li>
<li><p>R1 和 V3 都可以在 DeepSeek 官网上免费使用；API 的定价中，R1 输入部分的价格是 o1 的 1.82%，输出部分是 o1 的 3.65%；V3 输入部分的价格是 GPT-4o 的 1.12%，输出部分是 GPT-4o 的 2.8%。</p>
</li>
</ul>
</li>
<li><p>技术创新：DeepSeek-R1 的训练模式颠覆了常规认知。DeepSeek-R1 是首个验证了仅通过 RL（强化学习）无需 SFT （监督微调） 就能得到大幅推理能力增强和涌现的模型。这种训练方式大幅降低了数据标注成本，简化了训练流程，整体训练成本也得到了降低。</p>
</li>
<li><p>开源：目前没有其他在性能上对标 GPT-4o 和 o1 的开源模型。OpenAI 旗下主打的核心模型都没有开源，用户要使用必须通过APP或 API 调用。</p>
</li>
</ul>
<hr>
<h5 id="4-目前形势"><a href="#4-目前形势" class="headerlink" title="4. 目前形势"></a>4. 目前形势</h5><ul>
<li><p>中国AI公司做出真正的创新，美国科技大厂担心失去领先地位。</p>
<ul>
<li><p>在此之前，模型层面的技术革新虽然也并非罕见，但都是美国模型厂商率先推出、其他厂商跟进验证的节奏。这一次 DeepSeek 走到了前面。</p>
</li>
<li><p>DeepSeek 在模型训练和架构上都有创新，能够显著降低推理阶段的成本、提高效率。长期以来，AI发展依赖于计算能力的积累，可以说是超大规模者之间的竞赛。对比美国的竞争者，DeepSeek的创新实现了训练成本和使用价格上数量级的减少，美国公司领先市场的重要优势被削弱了。</p>
</li>
</ul>
</li>
<li><p>开源：生态若能星火燎原，将抢占美国公司市场。DeepSeek 的 R1 不仅通过技术报告公开了训练过程，还开源了模型的权重。DeepSeek的推理模型拥有高性能和低价格，使得开发者能将其用于越来越多的场景。最近，微软、英伟达、AWS都纷纷接入DeepSeek-R1。</p>
</li>
<li><p>大模型相关的美国科技股受到巨大冲击。英伟达股价大跌，似乎暗示了 DeepSeek 的真实威胁。因为DeepSeek 的路线一定程度上说明，无需最强算力也能训练出高性能大模型，而且 DeepSeek 把高性能模型开源的路线可能让更多公司放弃训练模型，冲击了英伟达核心算力产品（GPU）的需求，影响股价。并且，市场担忧 DeepSeek 的成功冲击 OpenAI 等美国重点科技公司的市场前景，尤其是闭源模型方向。</p>
</li>
</ul>
<hr>
<h5 id="5-DeepSeek未来可能迭代方向"><a href="#5-DeepSeek未来可能迭代方向" class="headerlink" title="5. DeepSeek未来可能迭代方向"></a>5. DeepSeek未来可能迭代方向</h5><ul>
<li><p>未来的创新点可能还是会围绕着<strong>成本、性能</strong>这两大核心要素。</p>
</li>
<li><p>多模态能力补齐。</p>
<ul>
<li>除夕凌晨，DeepSeek新发布的 DeepSeek-Janus-Pro 模型是一个多模态模型，同时拥有视觉理解和视觉生成的能力。但 Janus 系列模型都是小参数量模型，如何通过 Janus 创新的模型框架训练出一个大参数量的多模态模型，可能是未来的重点之一。</li>
</ul>
</li>
<li><p>DeepSeek 在2025年1月终于推出面向 C 端用户的 APP 产品，可能未来会探索&#x2F;合作更多应用。</p>
</li>
</ul>
<hr>
<h5 id="6-可能会带来的影响"><a href="#6-可能会带来的影响" class="headerlink" title="6. 可能会带来的影响"></a>6. 可能会带来的影响</h5><ul>
<li><p>国内AI公司面临进一步限制</p>
<ul>
<li><p>芯片制裁可能更严重：DeepSeek 的低成本训练成果，可能会让美国进一步收缩可供出口的芯片型号。未来，国内模型厂商可用的 GPU 型号越来越少，代际越来越旧。</p>
</li>
<li><p>模型和应用层面的封锁也会随之而来：由于隐私、数据合规等质疑，一些国家和地区已经要求 DeepSeek 停止服务。Twitter上，一些 AI 科普类博主从之前的无脑捧吹 DeepSeek 的帖子，已经转变为教用户 “如何本地化部署一个 DeepSeek R1 来保护自己的数据” 这样的帖子。</p>
</li>
</ul>
</li>
<li><p>全球AI生态的竞争可能会被重塑</p>
<ul>
<li>DeepSeek 得到市场认可，一定程度上说明，算法效率、经济高效将成为未来竞争中的核心要素。DeepSeek 正推动 AI 行业从“算力军备竞赛”转向“算法效率战争”，AI技术进一步普惠化。那些以往以“算力为重”的公司将要重新审视自己的战略。</li>
</ul>
</li>
<li><p>硅谷巨头们急迫重新领先</p>
<ul>
<li>技术上进行革新、重新夺取领先地位的紧迫感，会笼罩着美国的科技巨头们。据称，目前 Google、Apple、Meta 等公司，已经纷纷开始深度研究 DeepSeek。尽快推出下一个代际的领先模型，是硅谷各家的当务之急。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="7-可能面临的有利影响和发展方向"><a href="#7-可能面临的有利影响和发展方向" class="headerlink" title="7. 可能面临的有利影响和发展方向"></a>7. 可能面临的有利影响和发展方向</h5><p>有利影响：</p>
<ul>
<li><p>随着国际芯片制裁加剧，国内市场对国产GPU和AI芯片的需求将上升，有机会填补这一空白。</p>
</li>
<li><p>数据隐私和合规要求推动本地化部署，可提供定制化解决方案。</p>
</li>
<li><p>可优化算法与硬件的协同，提升整体性能。参与国产AI生态建设，推动从硬件到软件的自主可控。</p>
</li>
</ul>
<p>发展方向：提供定制化解决方案，针对数据隐私和合规需求，提供本地化部署支持。</p>
<hr>
<h5 id="8-技术细节"><a href="#8-技术细节" class="headerlink" title="8. 技术细节"></a>8. 技术细节</h5><h6 id="8-1-DeepSeek-V3-使用-MoE-架构："><a href="#8-1-DeepSeek-V3-使用-MoE-架构：" class="headerlink" title="8.1 DeepSeek-V3 使用 MoE 架构："></a>8.1 DeepSeek-V3 使用 MoE 架构：</h6><p>MoE (Mixture-of-Experts) 模型，即混合专家模型，是一种将多个小型专家模型组合起来，共同完成任务的架构。每个专家模型只处理部分输入，从而提高模型的效率和扩展性。</p>
<p>DeepSeek-V3 使用 MoE 架构的原因主要有以下几点：</p>
<ul>
<li>提高模型容量：MoE 可以显著增加模型的参数数量，而无需像稠密模型那样增加计算量。这使得模型可以学习更复杂的模式和关系，提高模型的性能。</li>
<li>降低训练成本：虽然 MoE 模型的总参数量很大，但每次激活的参数量却很小，从而减少了训练所需的计算资源。</li>
<li>提高推理效率： 在推理时，只有部分专家被激活，从而降低了推理的计算成本。</li>
</ul>
<hr>
<h6 id="8-2-DeepSeek-V3-使用Multi-Head-Latent-Attention-MLA"><a href="#8-2-DeepSeek-V3-使用Multi-Head-Latent-Attention-MLA" class="headerlink" title="8.2 DeepSeek-V3 使用Multi-Head Latent Attention (MLA)"></a>8.2 DeepSeek-V3 使用Multi-Head Latent Attention (MLA)</h6><p>Multi-Head Latent Attention (MLA)，即多头潜在注意力机制，旨在减少推理阶段的内存占用。MLA 和传统的 MHA 的主要区别在于：</p>
<ul>
<li>KV 缓存：在传统的 MHA 中，每个 token 的 Key 和 Value 向量都需要缓存起来，这在长文本生成中会占用大量的显存。而 MLA 通过低秩压缩技术，将 Key 和 Value 向量压缩成低维度的潜在向量，只需缓存压缩后的表示，从而显著减少 KV 缓存的内存占用。</li>
<li>通俗类比：想象在听一场演讲，传统的注意力机制需要记住每个时间点的所有细节（就像记录每一秒的录音）。而 MLA 则只提取关键信息（就像只记录每个重点句子），这样需要记忆的信息就大大减少了。</li>
</ul>
<p><img src="/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/2025-02-04-00-49-49-image.png"></p>
<hr>
<h6 id="8-3-DeepSeek-V3-的无辅助损失负载均衡策略"><a href="#8-3-DeepSeek-V3-的无辅助损失负载均衡策略" class="headerlink" title="8.3 DeepSeek-V3 的无辅助损失负载均衡策略"></a>8.3 DeepSeek-V3 的无辅助损失负载均衡策略</h6><p>在 MoE 模型中，如果专家负载不均衡，会导致以下问题：</p>
<ul>
<li>路由崩溃：某些专家可能会被过度使用，而其他专家则被闲置。</li>
<li>计算效率降低：过度使用的专家会成为性能瓶颈，而闲置的专家则浪费了计算资源。</li>
</ul>
<p>传统 MoE 模型为了实现专家间的负载均衡，通常引入辅助损失。但辅助损失会对模型的主要优化目标产生干扰，可能导致性能下降。为了解决这个问题，DeepSeek-V3 提出了无辅助损失的负载均衡策略。该策略的核心思想是：</p>
<ol>
<li>引入偏置项：为每个专家引入一个偏置项，并将这个偏置项添加到亲和度得分中，从而影响路由决策。</li>
<li>动态调整偏置项：在训练过程中，如果一个专家被过度使用，则减小其偏置项；如果一个专家被闲置，则增大其偏置项。这样，可以动态调整专家负载，使其达到均衡。</li>
<li>不使用辅助损失：偏置项只用于路由，而不影响最终的门控值计算，从而避免了传统辅助损失对模型性能的损害。</li>
</ol>
<p>类比： 想象在一个餐厅里，有多个厨师。如果顾客都点同样的菜，那么只有一个厨师会很忙，而其他厨师则没事做。为了解决这个问题，你可以告诉顾客：「今天点这道菜可以打折」，或者「点其他菜可以更快上菜」。这样，就可以引导顾客点不同的菜，让所有厨师都忙起来。DeepSeek-V3 的无辅助损失负载均衡策略就类似于这个引导顾客点不同菜的策略。</p>
<hr>
<h6 id="8-4-DeepSeek-V3-使用-Multi-Token-Prediction-MTP-提高模型性能"><a href="#8-4-DeepSeek-V3-使用-Multi-Token-Prediction-MTP-提高模型性能" class="headerlink" title="8.4 DeepSeek-V3 使用 Multi-Token Prediction (MTP) 提高模型性能"></a>8.4 DeepSeek-V3 使用 Multi-Token Prediction (MTP) 提高模型性能</h6><p>在传统的语言模型训练中，模型通常只预测下一个标记。MTP 则是在每个位置上，让模型预测多个未来的标记。</p>
<ul>
<li><p>实现方法：引入了多个预测模块，每个模块负责预测不同深度（未来第几个）的标记。在训练过程中，对每个深度的预测计算损失，然后将这些<strong>损失平均</strong>，作为整体的 MTP 损失。</p>
</li>
<li><p>好处：增加训练信号：通过预测多个 token，模型可以获得更密集的训练信号，提高数据效率。增强预规划能力：MTP 可以帮助模型更好地预先规划未来 token 的表示，使其更好地捕捉长距离依赖关系，从而提高模型的性能。</p>
</li>
</ul>
<hr>
<h6 id="8-5-DeepSeek-V3-使用-FP8-训练混合精度训练"><a href="#8-5-DeepSeek-V3-使用-FP8-训练混合精度训练" class="headerlink" title="8.5 DeepSeek-V3 使用 FP8 训练混合精度训练"></a>8.5 DeepSeek-V3 使用 FP8 训练混合精度训练</h6><p>在深度学习训练中，使用低精度浮点数（如 FP16、FP8）可以降低计算和存储需求，提高训练速度。但低精度可能导致数值不稳定和模型性能下降。</p>
<ul>
<li><p>解决方案：</p>
<ul>
<li><p>精细量化策略。细粒度量化：对张量进行小块划分（如 1×128 或 128×128），对每个小块单独计算缩放因子，进行量化，减少了量化误差带来的影响。</p>
</li>
<li><p>提高积累精度。增加乘加运算的精度：在进行矩阵乘法累加时，将部分结果提升到更高精度（如 FP32）进行积累，减少了因低精度引入的误差。</p>
</li>
<li><p>混合精度训练框架。保留关键操作的高精度：对于敏感的操作，如嵌入层、归一化层等，仍然使用高精度计算，确保训练的稳定性。</p>
</li>
</ul>
</li>
<li><p>DeepSeek-V3 使用 FP8 训练的主要原因是：</p>
<ul>
<li><p>加速训练：FP8 格式可以加速 GEMM (General Matrix Multiplication) 等核心计算操作，从而提高训练速度。</p>
</li>
<li><p>减少内存使用：FP8 格式可以显著减少内存占用，使得在有限的显存中训练更大的模型成为可能。</p>
</li>
<li><p>突破硬件限制：目前 NVIDIA H100 及以上的 GPU 已经对 FP8 有了较好的支持，而 DeepSeek-V3 使用了 FP8 混合精度训练，使其可以更好地利用硬件性能。</p>
</li>
</ul>
</li>
</ul>
<hr>
<h6 id="8-6-DeepSeek-V3-的-DualPipe-算法解决通信瓶颈"><a href="#8-6-DeepSeek-V3-的-DualPipe-算法解决通信瓶颈" class="headerlink" title="8.6 DeepSeek-V3 的 DualPipe 算法解决通信瓶颈"></a>8.6 DeepSeek-V3 的 DualPipe 算法解决通信瓶颈</h6><p>DualPipe 算法是 DeepSeek-V3 中用于解决通信瓶颈的一种高效流水线并行算法。它的核心思想是通过重叠前向和后向计算与通信，从而减少流水线气泡，提高训练效率。</p>
<p>具体来说，DualPipe 算法：</p>
<ol>
<li>双向流水线：同时从流水线的两端输入 micro-batch，实现双向并行计算。</li>
<li>计算-通信重叠：将每个 chunk 分成多个部分，并调整 GPU SM 用于通信和计算的比例，使得计算和通信可以相互重叠。</li>
<li>减少流水线气泡： 通过双向流水线和计算-通信重叠，减少了流水线中的等待时间，从而提高了训练效率。</li>
</ol>
<p>类比：可以将 DualPipe 算法想象成一个生产线。传统的流水线是按顺序一个一个地处理产品，而 DualPipe 算法是同时从两个方向处理产品，并让不同环节同时进行，从而提高了生产效率。</p>
<hr>
<h6 id="8-7-DeepSeek-V3-与传统的知识蒸馏的不同"><a href="#8-7-DeepSeek-V3-与传统的知识蒸馏的不同" class="headerlink" title="8.7 DeepSeek-V3 与传统的知识蒸馏的不同"></a>8.7 DeepSeek-V3 与传统的知识蒸馏的不同</h6><p>DeepSeek-V3 使用了一种特殊的知识蒸馏方法，从 DeepSeek-R1 模型中提炼推理能力。与传统的知识蒸馏不同，DeepSeek-V3 的知识蒸馏：</p>
<ul>
<li>重点在于推理模式：不是简单地模仿 R1 的输出，而是将 R1 的反思和验证模式融入到 DeepSeek-V3 中。</li>
<li>生成多样化的 SFT 数据：通过让 R1 生成带有反思和验证的响应，为 DeepSeek-V3 提供更高质量的 SFT 数据。</li>
<li>在强化学习中运用：在 RL 阶段，通过高温度采样生成融合了 R1 和原始数据的响应，使模型学习 R1 的推理模式。</li>
</ul>
<hr>
<h6 id="8-8-DeepSeek-V3-使用-tile-wise-和-block-wise-量化"><a href="#8-8-DeepSeek-V3-使用-tile-wise-和-block-wise-量化" class="headerlink" title="8.8 DeepSeek-V3 使用 tile-wise 和 block-wise 量化"></a>8.8 DeepSeek-V3 使用 <code>tile-wise</code> 和 <code>block-wise</code> 量化</h6><p><code>tile-wise</code> 和 <code>block-wise</code> 量化是用于提高 FP8 量化精度的两种细粒度量化方法：</p>
<ul>
<li>tile-wise 量化：对于激活值（activations），将 1xNe 的元素分组成一个 tile，然后在 tile 内进行缩放（scaling）。这里的 Ne 通常是 128，所以就是一个 1x128 的 tile。这意味着，对于每一个 token，每 128 个通道（channel）会被分成一组。</li>
<li>block-wise 量化：对于权重（weights），将 Ne x Ne 的元素分组成一个 block，然后在 block 内进行缩放。这里的 Ne 也通常是 128，所以就是一个 128x128 的 block。这意味着，每 128 个输入通道和 128 个输出通道会被分组成一个 block。</li>
</ul>
<p>这种细粒度的量化方法可以更好地适应激活值和权重中的异常值，提高量化的精度，从而使模型能够更有效地利用低精度的 FP8 格式进行训练。</p>
<p>类比：可以将 <code>tile-wise</code> 和 <code>block-wise</code> 量化想象成在打包货物时，不是把所有货物都放在一个大箱子里，而是将它们根据形状、大小等进行分类，然后分别打包。这样可以更好地利用空间，减少货物之间的挤压和碰撞。</p>
<hr>
<h6 id="8-9-DeepSeek-R1-Zero-研究纯强化学习（RL）在-LLM-推理能力上的应用"><a href="#8-9-DeepSeek-R1-Zero-研究纯强化学习（RL）在-LLM-推理能力上的应用" class="headerlink" title="8.9 DeepSeek-R1-Zero 研究纯强化学习（RL）在 LLM 推理能力上的应用"></a>8.9 DeepSeek-R1-Zero 研究纯强化学习（RL）在 LLM 推理能力上的应用</h6><p>直接在基础模型上应用强化学习，不使用任何 SFT 数据。探索 LLM 在纯 RL 环境下的自演化过程，使其自主发展推理能力。</p>
<ul>
<li><p>近年来，LLM 在各个领域都取得了显著进展，但推理能力仍有提升空间。</p>
</li>
<li><p>之前的研究大多依赖于大量的监督式微调（SFT）数据，但获取高质量的 SFT 数据成本高昂。</p>
</li>
<li><p>OpenAI 的 o1 系列模型通过增加思维链（Chain-of-Thought, CoT）推理过程的长度来提升推理能力，但如何有效进行测试时（test-time）扩展仍是开放问题。</p>
</li>
<li><p>一些研究尝试使用基于过程的奖励模型、强化学习和搜索算法来解决推理问题，但没有达到 OpenAI 的 o1 系列模型的通用推理性能水平。</p>
</li>
</ul>
<hr>
<h6 id="8-10-DeepSeek-R1-Zero-的「顿悟」时刻-aha-moment"><a href="#8-10-DeepSeek-R1-Zero-的「顿悟」时刻-aha-moment" class="headerlink" title="8.10 DeepSeek-R1-Zero 的「顿悟」时刻 aha moment"></a>8.10 DeepSeek-R1-Zero 的「顿悟」时刻 aha moment</h6><p>在大规模强化学习中，模型的「思考过程」会不断与最终的正确率奖励相互作用。当模型最初得出的答案并未得到较高奖励时，它会在后续的推理中「回头反省」，尝试补充或修正先前的思路，从而获得更高的奖励。随着强化学习的迭代，这种「主动回溯、推翻先前想法并重新推理」的行为逐渐巩固，便在输出中表现为所谓的「aha moment」。本质上，这是RL为模型「留出了」足够的思考和试错空间，当模型自行发现更优思路时，就会出现类似人类「恍然大悟」的瞬间。</p>
<p>这也展示了 RL 的强大潜力，它可以让模型在没有明确指导的情况下，自主学习并改进。</p>
<hr>
<h6 id="8-11-DeepSeek-R1相比DeepSeek-R1-Zero-做出的改进"><a href="#8-11-DeepSeek-R1相比DeepSeek-R1-Zero-做出的改进" class="headerlink" title="8.11 DeepSeek-R1相比DeepSeek-R1-Zero 做出的改进"></a>8.11 DeepSeek-R1相比DeepSeek-R1-Zero 做出的改进</h6><ul>
<li><p>引入冷启动数据：使用了数千条带详细推理过程（长CoT）的数据先做一次SFT，让模型初始时就具备一定可读性与写作风格。</p>
</li>
<li><p>分阶段RL：在推理收敛后，通过拒绝采样等手段获得更多优质监督样本，再进行SFT，再全场景RL，不断修正模型的正确性与通用能力。</p>
</li>
<li><p>语言一致性奖励：避免模型出现大量的拼写或中英文混杂，从而保证可读性。</p>
</li>
</ul>
<hr>
<h6 id="8-12-DeepSeek-R1-要使用冷启动数据"><a href="#8-12-DeepSeek-R1-要使用冷启动数据" class="headerlink" title="8.12 DeepSeek-R1 要使用冷启动数据"></a>8.12 DeepSeek-R1 要使用冷启动数据</h6><p>DeepSeek-R1 使用冷启动数据的主要目的是为了解决 DeepSeek-R1-Zero 在训练早期出现的训练不稳定问题。相比于直接在基础模型上进行 RL，使用少量的 SFT 数据进行冷启动，可以让模型更快地进入稳定训练阶段：</p>
<ul>
<li>可读性：冷启动数据使用更易于理解的格式，输出内容更适合人类阅读，避免了 DeepSeek-R1-Zero 输出的语言混合、格式混乱等问题。</li>
<li>潜在性能：通过精心设计冷启动数据的模式，可以引导模型产生更好的推理能力。</li>
<li>稳定训练：使用 SFT 数据作为起始点，可以避免RL 训练早期阶段的不稳定问题。</li>
</ul>
<hr>
<h6 id="8-13-DeepSeek-R1-的多阶段训练框架中每个阶段的侧重点"><a href="#8-13-DeepSeek-R1-的多阶段训练框架中每个阶段的侧重点" class="headerlink" title="8.13 DeepSeek-R1 的多阶段训练框架中每个阶段的侧重点"></a>8.13 DeepSeek-R1 的多阶段训练框架中每个阶段的侧重点</h6><ul>
<li><p>冷启动（Cold Start）：使用少量高质量的 CoT 数据对基础模型进行微调，作为 RL 训练的初始起点。侧重点是让模型掌握基本的 CoT 推理能力，并使模型的输出更具可读性。</p>
</li>
<li><p>推理导向的强化学习（Reasoning-oriented RL）：在冷启动模型的基础上进行 RL 训练，侧重点是提升模型在推理任务上的性能。在这个阶段，会引入语言一致性奖励，以减少推理过程中的语言混合问题。</p>
</li>
<li><p>拒绝采样和监督微调（Rejection Sampling and SFT）：使用上一阶段的 RL 模型进行拒绝采样，生成高质量的推理和非推理数据，并用这些数据对模型进行微调。侧重点是提升模型的综合能力，使其在写作、事实问答等多种任务上表现良好。</p>
</li>
<li><p>所有场景下的强化学习（RL for all scenarios）：在上一阶段 SFT 模型的基础上进行 RL 训练，侧重点是使模型在所有场景下都能表现良好，包括推理任务和非推理任务，并且保证模型的安全性和无害性。</p>
</li>
</ul>
<hr>
<h6 id="8-14-DeepSeekMath-Corpus-的构建过程，需要迭代式地收集数据"><a href="#8-14-DeepSeekMath-Corpus-的构建过程，需要迭代式地收集数据" class="headerlink" title="8.14 DeepSeekMath Corpus 的构建过程，需要迭代式地收集数据"></a>8.14 DeepSeekMath Corpus 的构建过程，需要迭代式地收集数据</h6><p>迭代式地收集数据可以帮助不断优化 FastText 模型，从而发现更多高质量的数学网页。</p>
<ul>
<li>第一轮，使用 OpenWebMath 作为种子数据训练了一个 FastText 模型。但是这个模型只学到了一部分数学网页的特征，还有很多数学网页没有被识别出来。</li>
<li>为了让 FastText 模型更强大，分析第一轮收集到的网页的来源，找到一些可能包含更多数学网页的网站，比如 <a href="https://link.zhihu.com/?target=http://mathoverflow.net">http://mathoverflow.net</a>。然后，人工标注这些网站中的数学网页，将他们加入到种子数据中，用新的种子数据训练新的 FastText 模型，这个新模型就可以识别更多的数学网页。</li>
<li>通过多轮迭代，FastText 模型越来越强大，可以找到更多高质量的数学网页。就像我们学习一个新知识一样，先从简单的概念入手，然后不断深入，才能全面掌握。</li>
</ul>
<hr>
<h6 id="8-15-DeepSeekMath-Base-从-DeepSeek-Coder-Base-模型开始进行数学预训练"><a href="#8-15-DeepSeekMath-Base-从-DeepSeek-Coder-Base-模型开始进行数学预训练" class="headerlink" title="8.15 DeepSeekMath-Base 从 DeepSeek-Coder-Base 模型开始进行数学预训练"></a>8.15 DeepSeekMath-Base 从 DeepSeek-Coder-Base 模型开始进行数学预训练</h6><p>使用 DeepSeek-Coder-Base-v1.5 7B 作为初始化模型，并在 DeepSeekMath Corpus 上进行预训练。因为发现代码训练有助于提升模型的推理能力，特别是数学推理能力。具体来说，代码训练可以让模型更好地理解逻辑和结构化的思维，这对于解决数学问题至关重要。先进行代码训练，再进行数学训练，可以获得更好的数学推理能力。</p>
<ul>
<li>逻辑思维：代码的执行需要精确的逻辑，这与数学推理非常相似。代码训练可以帮助模型学习和掌握逻辑思维的能力，从而更好地解决数学问题。</li>
<li>结构化思维：代码具有严谨的结构，这与数学证明的结构类似。代码训练可以让模型更好地理解和运用结构化思维，这有助于模型更好地分析和解决数学问题。</li>
<li>抽象能力：代码中的变量和函数是对现实世界事物的抽象，这与数学中的符号和公式类似。代码训练可以帮助模型提高抽象能力，从而更好地理解数学概念和公式。</li>
<li>解决问题的能力：代码的目的是解决问题，数学的目的是解决数学问题，这个过程都需要很强的解决问题的能力，代码训练可以帮助模型提高解决问题的能力，从而更好地解决数学问题。</li>
</ul>
<hr>
<h6 id="8-16-DeepSeekMath-RL-通过-GRPO-强化学习进一步优化的模型"><a href="#8-16-DeepSeekMath-RL-通过-GRPO-强化学习进一步优化的模型" class="headerlink" title="8.16 DeepSeekMath-RL 通过 GRPO 强化学习进一步优化的模型"></a>8.16 DeepSeekMath-RL 通过 GRPO 强化学习进一步优化的模型</h6><p>GRPO 算法减少计算资源消耗。GRPO 的核心在于它避免了像 PPO 那样训练一个额外的 value 模型，而是通过 group scores 来估计 baseline。</p>
<ul>
<li><p>PPO 需要训练一个额外的 value 模型来估计状态的价值，这个 value 模型通常和 policy 模型规模相当，所以训练过程很耗费计算资源。</p>
</li>
<li><p>GRPO 则不同，它不需要训练额外的 value 模型。对于每个问题，GRPO 会从 policy 模型中采样多个输出，然后将这些输出的奖励值进行归一化，并以此作为 baseline。这样，GRPO 可以省去训练 value 模型的开销，从而大大减少计算资源的消耗。</p>
</li>
<li><p>更形象地说，PPO 要给每个学生都找一个辅导老师（value model），而 GRPO 是让学生之间互相评价（group score），然后进行自我调整，显然后者更节省资源。</p>
</li>
</ul>
<p><img src="/2025/02/03/13-DeepSeek%E6%80%BB%E7%BB%93/2025-02-04-01-33-36-image.png"></p>
<hr>
<h6 id="8-17-RL-的未来方向"><a href="#8-17-RL-的未来方向" class="headerlink" title="8.17 RL 的未来方向"></a>8.17 RL 的未来方向</h6><p>总的来说，未来的研究方向将集中于如何让 RL 更有效地利用数据，更可靠地学习，以及更准确地评估。</p>
<ul>
<li><p>数据源：</p>
<ul>
<li><p>探索更多样的训练数据，包括分布外的问题和高级解码策略；</p>
</li>
<li><p>优化模型探索效率。</p>
</li>
</ul>
</li>
<li><p>算法：</p>
<ul>
<li>开发更鲁棒的 RL 算法，以应对训练信号中的噪声，比如使用弱监督方法。</li>
</ul>
</li>
<li><p>奖励函数：</p>
<ul>
<li><p>提高奖励函数的泛化能力，使其能够处理分布外问题和高级解码输出；</p>
</li>
<li><p>反映奖励函数的不确定性，将其与弱监督方法连接起来；</p>
</li>
<li><p>构建高质量的过程奖励模型，为推理过程提供细粒度的训练信号。</p>
</li>
</ul>
</li>
</ul>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/01/27/12-Docker%E5%AE%B9%E5%99%A8%E4%B8%AD%E8%A7%A3%E5%86%B3SharedMemory%E4%B8%8D%E8%B6%B3%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/27/12-Docker%E5%AE%B9%E5%99%A8%E4%B8%AD%E8%A7%A3%E5%86%B3SharedMemory%E4%B8%8D%E8%B6%B3%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">12-Docker容器中解决Shared Memory不足问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-01-27 11:49:05 / 修改时间：14:45:51" itemprop="dateCreated datePublished" datetime="2025-01-27T11:49:05+08:00">2025-01-27</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <hr>
<h5 id="1-参考"><a href="#1-参考" class="headerlink" title="1, 参考"></a>1, 参考</h5><ul>
<li><a target="_blank" rel="noopener" href="https://gitee.com/ascend/ascend-docker-image/tree/dev/mindie#%E5%90%AF%E5%8A%A8%E5%AE%B9%E5%99%A8">启动容器</a></li>
</ul>
<h5 id="1-报错信息"><a href="#1-报错信息" class="headerlink" title="1. 报错信息"></a>1. 报错信息</h5><p>在使用 PyTorch 等深度学习框架时，可能会遇到以下错误：</p>
<figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">RuntimeError:</span> DataLoader worker (pid X) <span class="built_in">is</span> killed <span class="keyword">by</span> signal: Bus <span class="keyword">error</span>. It <span class="built_in">is</span> possible that dataloader<span class="comment">&#x27;s workers are out of shared memory...</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p><strong>错误原因</strong></p>
</blockquote>
<blockquote>
<p>这个错误通常是由于容器的共享内存（<code>/dev/shm</code>）不足引起的，尤其在处理大规模数据加载或多线程操作时容易出现。</p>
</blockquote>
<hr>
<h5 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="2. 解决方案"></a>2. 解决方案</h5><p>增大共享内存大小 (<code>--shm-size</code>)</p>
<p>在使用 Docker 启动容器时，通过 <code>--shm-size</code> 参数增大共享内存。例如，将共享内存设置为 8GB：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --shm-size=8g ...</span><br></pre></td></tr></table></figure>

<p>可以使用以下命令查看容器的共享内存使用情况：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">df</span> -h /dev/shm</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="3-操作步骤"><a href="#3-操作步骤" class="headerlink" title="3. 操作步骤"></a>3. 操作步骤</h5><p><strong>针对现有容器修改共享内存大小</strong></p>
<p>在 Docker 中，共享内存大小属于<strong>容器运行期配置</strong>，无法直接对已创建的容器进行修改。为了保存现有容器内容并重新配置共享内存，可以按照以下步骤进行。</p>
<ul>
<li>停止运行中的容器</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br><span class="line">docker stop &lt;容器ID&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>使用 <code>docker commit</code> 保存容器</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit &lt;容器ID&gt; &lt;new-image-name&gt;:&lt;tag&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>以新的共享内存大小启动容器</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -d --net=host --shm-size=8g \</span><br><span class="line">    --privileged \</span><br><span class="line">    --name &lt;new-container-name&gt; \</span><br><span class="line">    --device=/dev/davinci_manager \</span><br><span class="line">    --device=/dev/hisi_hdc \</span><br><span class="line">    --device=/dev/devmm_svm \</span><br><span class="line">    --device=/dev/davinciX \</span><br><span class="line">    -v /usr/local/Ascend/driver:/usr/local/Ascend/driver:ro \</span><br><span class="line">    -v /usr/local/sbin:/usr/local/sbin:ro \</span><br><span class="line">    -v /home/ljw:/home/ljw \</span><br><span class="line">    &lt;new-image-name&gt;:&lt;tag&gt; bash</span><br></pre></td></tr></table></figure>

<ul>
<li>验证共享内存大小</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">df</span> -h /dev/shm</span><br></pre></td></tr></table></figure>

<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/01/23/11-%E7%90%86%E8%A7%A3GridSample/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/23/11-%E7%90%86%E8%A7%A3GridSample/" class="post-title-link" itemprop="url">11-理解GridSample</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-01-23 15:10:11" itemprop="dateCreated datePublished" datetime="2025-01-23T15:10:11+08:00">2025-01-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-11 16:13:27" itemprop="dateModified" datetime="2025-02-11T16:13:27+08:00">2025-02-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在 PyTorch 中，&#96;&#96;torch.nn.functional.grid_sample&#96; 用于基于给定的“采样网格”（grid）对输入图像或特征图进行插值采样，从而实现对输入数据的空间变换。</p>
<hr>
<h5 id="1-参考"><a href="#1-参考" class="headerlink" title="1. 参考"></a>1. 参考</h5><ul>
<li><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html">torch.nn.functional.grid_sample &mdash; PyTorch 2.5 documentation</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/opdevg/Ascendcopdevg/atlas_ascendc_10_0001.html">Ascend C简介-Ascend C算子开发-算子开发-CANN商用版8.0.0开发文档-昇腾社区</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch/blob/main/torch/nn/functional.py">pytorch&#x2F;torch&#x2F;nn&#x2F;functional.py at main · pytorch&#x2F;pytorch · GitHub</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://gitee.com/ascend/DrivingSDK/blob/master/docs/api/context/grid_sampler2d_v2%5Bbeta%5D.md">docs&#x2F;api&#x2F;context&#x2F;grid_sampler2d_v2[beta].md · Ascend&#x2F;DrivingSDK - Gitee.com</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://gitee.com/ascend/DrivingSDK/blob/master/kernels/op_host/grid_sampler2d_v2.cpp">kernels&#x2F;op_host&#x2F;grid_sampler2d_v2.cpp · Ascend&#x2F;DrivingSDK - Gitee.com</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://gitee.com/ascend/DrivingSDK/blob/master/kernels/op_host/grid_sampler2d_v2_tiling.h">kernels&#x2F;op_host&#x2F;grid_sampler2d_v2_tiling.h · Ascend&#x2F;DrivingSDK - Gitee.com</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://gitee.com/ascend/DrivingSDK/blob/master/kernels/op_kernel/grid_sampler2d_v2.cpp">kernels&#x2F;op_kernel&#x2F;grid_sampler2d_v2.cpp · Ascend&#x2F;DrivingSDK - Gitee.com</a></p>
</li>
</ul>
<hr>
<h5 id="2-功能概述"><a href="#2-功能概述" class="headerlink" title="2. 功能概述"></a>2. 功能概述</h5><ul>
<li>核心逻辑：</li>
</ul>
<blockquote>
<p>根据输入的采样网格（grid），对输入特征图（input）进行插值操作，得到变换后的输出特征图（output）。</p>
</blockquote>
<ul>
<li>常见应用场景：</li>
</ul>
<blockquote>
<p><strong>空间变换网络（Spatial Transformer Networks, STN）</strong>：配合 <code>affine_grid</code>，构建端到端可微分的空间变换模块。</p>
<p><strong>图像变换和对齐</strong>：对输入图像进行旋转、缩放、平移等操作。</p>
<p><strong>任意插值采样</strong>：基于自定义网格从输入中提取特征。</p>
</blockquote>
<hr>
<h5 id="3-GradSample2D算子设计规格"><a href="#3-GradSample2D算子设计规格" class="headerlink" title="3. GradSample2D算子设计规格"></a>3. GradSample2D算子设计规格</h5><table>
<thead>
<tr>
<th>Classify</th>
<th>Name</th>
<th>Type</th>
<th>TypeRangeAll</th>
<th>Attr_Default_value</th>
<th>Format</th>
</tr>
</thead>
<tbody><tr>
<td>INPUT</td>
<td>input</td>
<td>tensor</td>
<td>fp32,fp16</td>
<td>-</td>
<td>ND</td>
</tr>
<tr>
<td>INPUT</td>
<td>grid</td>
<td>tensor</td>
<td>fp32,fp16</td>
<td>-</td>
<td>ND</td>
</tr>
<tr>
<td>INPUT</td>
<td>mode</td>
<td>string</td>
<td>-</td>
<td>bilinear</td>
<td></td>
</tr>
<tr>
<td>INPUT</td>
<td>padding_mode</td>
<td>string</td>
<td>-</td>
<td>zeros</td>
<td></td>
</tr>
<tr>
<td>INPUT</td>
<td>align_corners</td>
<td>bool</td>
<td>bool</td>
<td>FALSE</td>
<td></td>
</tr>
<tr>
<td>OUTPUT</td>
<td>output</td>
<td>tensor</td>
<td>fp32,fp16</td>
<td>-</td>
<td>ND</td>
</tr>
</tbody></table>
<ul>
<li><p><strong>输入与输出</strong>：</p>
<ul>
<li><p><code>input</code>：输入张量，形状为 (N, C, Hin, Win)</p>
</li>
<li><p><code>grid</code>：采样网格，形状为 (N, Hout, Wout, 2)，其中最后一个维度表示网格的二维坐标 (x, y)</p>
</li>
<li><p><code>mode</code>：插值方式，默认使用双线性插值。</p>
<ul>
<li>‘bilinear’：双线性插值（常用于大多数需要平滑变换的情景）。</li>
<li>‘nearest’：最邻近插值。</li>
<li>‘bicubic&#96;：双三次插值（仅支持 4D 输入）。</li>
</ul>
</li>
<li><p><code>padding_mode</code>：当网格坐标超出 [−1,1] 范围时的填充方式，默认用 0 填充。</p>
<ul>
<li>‘zeros’：超出范围的点直接赋 0。</li>
<li>‘border’：使用边界像素进行填充。</li>
<li>‘reflection’：将超出边界的坐标点“反射”回图像内部。</li>
</ul>
</li>
<li><p><code>align_corners</code>：控制 [−1,1] 坐标是否对齐到像素中心，默认为False。</p>
<ul>
<li>True：对齐到中心，适用于对某些几何变换有严格对齐需求的场景，但会带来分辨率依赖性。</li>
<li>False：对齐到边界，分辨率无关</li>
</ul>
</li>
<li><p><code>output</code>：输出采样之后的特征图，形状为 (N,C,Hout​,Wout​)。</p>
</li>
</ul>
</li>
<li><p><strong>核函数名称</strong>：grad_sample_2D_custom</p>
</li>
<li><p><strong>使用的主要接口</strong></p>
<ul>
<li>DataCopy：数据搬移接口</li>
<li>AllocTensor、FreeTensor：内存管理接口</li>
<li>EnQue、DeQue接口：Queue队列管理接口</li>
</ul>
</li>
<li><p><strong>算子实现文件名称</strong>：grad_sample_2D_custom.cpp</p>
</li>
<li><p>此函数通常与<code>affine_grid</code>一起使用以构建空间变换网络</p>
</li>
</ul>
<hr>
<h5 id="4-工作原理"><a href="#4-工作原理" class="headerlink" title="4. 工作原理"></a>4. 工作原理</h5><p>PyTorch 的 <code>grid_sample</code> 在 Forward 和 Backward 两个方向上都有清晰的实现逻辑。</p>
<ul>
<li><strong>Forward 过程</strong>：</li>
</ul>
<blockquote>
<p><strong>网格生成</strong>（配合 affine_grid）：</p>
<ul>
<li>将标准化网格坐标 [−1,1] 映射到输入图像坐标系。</li>
</ul>
<p><strong>采样</strong>：</p>
<ul>
<li><p>遍历输出图像的每个像素，根据 grid 提供的坐标从输入图像中插值。</p>
</li>
<li><p>插值方式由 mode 决定，支持双线性插值（bilinear）、最近邻插值（nearest）和双三次插值（bicubic）。</p>
</li>
</ul>
<p><strong>越界处理</strong>：</p>
<ul>
<li><p>对超出输入范围的坐标，根据 padding_mode 的设置进行填充。</p>
</li>
<li><p>常见的填充方式包括填充 0、使用边界值或反射回输入区域。</p>
</li>
</ul>
<p><strong>输出生成</strong>：</p>
<ul>
<li>按照采样结果生成新的特征图。</li>
</ul>
</blockquote>
<ul>
<li><strong>Backward 过程</strong>：</li>
</ul>
<blockquote>
<p><strong>对输入特征图的梯度计算</strong>：</p>
<ul>
<li>输出梯度根据插值权重反向传播到输入特征图。</li>
</ul>
<p><strong>对采样网格的梯度计算</strong>：</p>
<ul>
<li>输出梯度中的坐标相关部分反向传播到 grid，从而更新控制网格生成的参数。</li>
</ul>
</blockquote>
<hr>
<h5 id="5-代码实现细节-Python-前端接口"><a href="#5-代码实现细节-Python-前端接口" class="headerlink" title="5. 代码实现细节-Python 前端接口"></a>5. 代码实现细节-Python 前端接口</h5><p><strong>Python 前端 API</strong> (torch.nn.functional.grid_sample)</p>
<ul>
<li><p>位于<code>pytorch-main\torch\nn\functional.py</code>中</p>
</li>
<li><p>这是在 Python 里调用的函数接口，实际上会转到 C++&#x2F;CUDA 端去执行，参数检查、文档字符串等都在这里完成。</p>
</li>
<li><p>核心函数：</p>
<ul>
<li><p><strong>affine_grid</strong> 负责“创建”变换后每个输出像素在输入图像&#x2F;特征图中对应的坐标</p>
</li>
<li><p><strong>grid_sample</strong> 则根据这些坐标，在输入中插值得到最终变换后的输出。</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2025/01/23/11-%E7%90%86%E8%A7%A3GridSample/2025-01-22-17-47-29-image.png"></p>
<blockquote>
<p>函数整体流程：</p>
<p><strong>affine_grid(theta, size, align_corners)</strong></p>
<ol>
<li><p>首先根据 size 计算输出网格的每个坐标点，其范围是 [−1,1]。</p>
</li>
<li><p>用仿射矩阵 θ 把这些标准坐标映射到输入图像坐标系中。</p>
</li>
<li><p>结果就是网格张量，每个位置告诉你“输出图像的这个像素，对应输入图像中的哪一点”。</p>
</li>
</ol>
<p><strong>grid_sample(input, grid, mode, padding_mode, align_corners)</strong></p>
<ol>
<li><p>遍历输出图像上的每个像素位置，从 grid 取到对应输入图上的坐标点。</p>
</li>
<li><p>根据 mode 所指定的插值方法，去输入图 input 中拿到像素值并插值。</p>
</li>
<li><p>如果坐标越界，就根据 padding_mode 做处理（填零、取边界或镜像）。</p>
</li>
<li><p>逐像素拼出采样后的输出图。</p>
</li>
</ol>
</blockquote>
<hr>
<h5 id="6-代码实现细节-C-CUDA-层实现"><a href="#6-代码实现细节-C-CUDA-层实现" class="headerlink" title="6. 代码实现细节-C++ &#x2F; CUDA 层实现"></a>6. 代码实现细节-C++ &#x2F; CUDA 层实现</h5><p><strong>C++&#x2F;ATen 层</strong>（主要逻辑入口）</p>
<p>在 <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch">PyTorch GitHub 仓库</a> 中，核心的 grid_sample C++ 入口主要位于</p>
<ul>
<li><code>aten/src/ATen/native/GridSampler.cpp</code>（CPU 逻辑）</li>
<li><code>aten/src/ATen/native/cuda/GridSampler.cu</code>（CUDA 逻辑）</li>
<li>以及一些与调度、类型分发相关的文件。</li>
</ul>
<p><img src="/2025/01/23/11-%E7%90%86%E8%A7%A3GridSample/2025-01-23-14-32-34-image.png"></p>
<p><img src="/2025/01/23/11-%E7%90%86%E8%A7%A3GridSample/2025-01-23-14-33-27-image.png"></p>
<p><strong>forward&#x2F;backward 内核（kernel）</strong></p>
<ul>
<li><strong>Forward</strong>：在 GridSampler.cpp &#x2F; GridSampler.cu中，会分别实现 grid_sampler_2d_fwd_kernel、grid_sampler_3d_fwd_kernel 等（对应 2D、3D 输入）</li>
<li><strong>Backward</strong>：相应地，也会有 grid_sampler_2d_bwd_kernel、grid_sampler_3d_bwd_kernel 来计算反向传播时对输入图像及 grid 的梯度。</li>
</ul>
<p><strong>grid_sampler 在 ATen 层的函数签名</strong></p>
<ul>
<li><p>位于<code>aten\src\ATen\native\native_functions.yaml</code></p>
</li>
<li><p>声明了 grid_sampler 在 ATen 层的函数签名。</p>
</li>
<li><p>PyTorch 会根据这里的信息生成对外暴露的 C++&#x2F;Python 绑定。</p>
</li>
</ul>
<p><img src="/2025/01/23/11-%E7%90%86%E8%A7%A3GridSample/2025-01-23-14-51-40-image.png"></p>
<blockquote>
<p>主要流程：</p>
<p><strong>参数校验与维度检查</strong>：</p>
<ul>
<li><p>确保输入和网格的维度匹配。</p>
</li>
<li><p>检查 mode和 padding_mode 的合法性。</p>
</li>
</ul>
<p><strong>网格遍历与插值</strong>：</p>
<ul>
<li><p>遍历输出图像的每个像素，从网格中获取对应的输入坐标。</p>
</li>
<li><p>根据 mode 插值并生成输出像素值。</p>
</li>
</ul>
<p><strong>GPU 加速</strong>：</p>
<ul>
<li>根据输出尺寸，动态计算线程块和线程布局，调用 CUDA 内核实现并行处理。</li>
</ul>
</blockquote>
<hr>
<h5 id="7-主要逻辑"><a href="#7-主要逻辑" class="headerlink" title="7. 主要逻辑"></a>7. 主要逻辑</h5><p>PyTorch 在 C++&#x2F;CUDA 层的 grid_sample 实现大致可拆分为以下步骤：</p>
<ul>
<li><p><strong>维度解析与参数检查</strong></p>
<ul>
<li><p>检查输入 input 和 grid 的维度是否匹配（如 4D vs. 4D 网格，5D vs. 5D 网格等）。</p>
</li>
<li><p>检查插值模式（bilinear, nearest, bicubic）和边界模式（zeros, border, reflection）是否合法。</p>
</li>
<li><p>解析 align_corners 标志，后续计算插值坐标时需要用到。</p>
</li>
</ul>
</li>
<li><p><strong>确定并行策略</strong></p>
<ul>
<li><p>在 CPU 端通常使用 OpenMP 或者其他多线程方式并行遍历输出像素；</p>
</li>
<li><p>在 GPU（CUDA）端，会依据输出的尺寸，计算合适的线程块（block）和线程（thread）布局，并发起 kernel 调用。</p>
</li>
</ul>
</li>
<li><p><strong>在内核中遍历输出像素</strong></p>
<ul>
<li><p><strong>核心思路</strong>：对输出空间中的每个像素 (n,c,h,w)（或 3D 时 (n,c,d,h,w)）来说，先从 grid[n, h, w] 读取到输入特征图上的坐标 (x,y)，然后：</p>
<ol>
<li>根据插值模式 (mode) 计算该坐标对应的输入像素值。</li>
<li>如果坐标超出 [−1,1]，则按照 padding_mode（如 zeros&#x2F;border&#x2F;reflection）处理。</li>
<li>将得到的插值结果写入输出张量的相应位置。</li>
</ol>
</li>
<li><p>对于插值模式，“bilinear” 在 2D 场景下需要对 4 个邻近像素（左上、右上、左下、右下）做双线性加权，而 “nearest” 则直接取最近的像素，“bicubic” 做 4x4 neighborhood 的三次插值，3D 时思路类似，只是变为三线性、最邻近或三次插值。</p>
</li>
</ul>
</li>
<li><p><strong>反向传播</strong>（Backward）</p>
<ul>
<li><p>如果在计算梯度（即调用 .backward()），grid_sample 还需对输入 input 和 grid 计算梯度：</p>
<ol>
<li>对 input 的梯度：将来自输出像素的梯度<strong>按照插值权重</strong>反向加到输入对应像素上。</li>
<li>对 grid 的梯度：同样使用插值的偏导（∂x^&#x2F;∂x 等）将输出梯度中与坐标相关的部分累加到 grid。</li>
</ol>
</li>
<li><p>这就保证了在训练基于 grid_sample 的网络（例如空间变换网络 STN）时，θ（或任何定义 grid 的参数）可以通过可微分插值把梯度传回来。</p>
</li>
</ul>
</li>
</ul>
<blockquote>
<p><strong>核心算法</strong>就是<strong>对输出像素进行遍历</strong>，从网格中取到输入坐标，根据插值模式去输入张量读值并插值，然后写回输出；反向传播还要计算对输入和网格的梯度。</p>
</blockquote>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/01/23/10-AscendC%E7%8E%AF%E5%A2%83%E4%B8%8B%E8%B0%83%E8%AF%95%E6%89%8B%E6%AE%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/23/10-AscendC%E7%8E%AF%E5%A2%83%E4%B8%8B%E8%B0%83%E8%AF%95%E6%89%8B%E6%AE%B5/" class="post-title-link" itemprop="url">10-Ascend C环境下调试手段</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-01-23 11:49:05" itemprop="dateCreated datePublished" datetime="2025-01-23T11:49:05+08:00">2025-01-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-07 16:36:32" itemprop="dateModified" datetime="2025-02-07T16:36:32+08:00">2025-02-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>下面是一份对「Ascend C」环境下常用调试手段的总结，涵盖从主机侧（Host）到核函数（Kernel）内的不同调试方法，以及日志排查与编译期调试信息输出等维度。可以根据实际需求自由组合使用。</p>
<hr>
<h5 id="1-host侧"><a href="#1-host侧" class="headerlink" title="1. host侧"></a>1. host侧</h5><ul>
<li>打印简单变量：直接使用 C++ <code>std::cout</code> 输出变量值，便于观察在 Host 端的数值变化：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">std::cout &lt;&lt; &quot;kernelHeight: &quot; &lt;&lt; kernelHeight &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure>

<ul>
<li>打印数组：编写一个通用的打印函数，循环输出数组元素：</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">void printArray(const uint32_t* array, uint32_t size) &#123;</span><br><span class="line">    for (uint32_t i = 0; i &lt; size; ++i) &#123;</span><br><span class="line">        std::cout &lt;&lt; array[i] &lt;&lt; &quot; &quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">        std::cout &lt;&lt; &quot;seqHeight: &quot;;</span><br><span class="line">        printArray(seqHeight, outHeight);</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; &quot;seqWidth: &quot;;</span><br><span class="line">        printArray(seqWidth, outWidth);</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="2-Kernel-侧打印"><a href="#2-Kernel-侧打印" class="headerlink" title="2. Kernel 侧打印"></a>2. Kernel 侧打印</h5><p>在核函数（Kernel）内部，可以使用 <code>PRINTF</code> 函数输出常量或变量的调试信息。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int32_t a = heightSeq[0];</span><br><span class="line">PRINTF(&quot;【a %d 】  &quot;, a);</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="3-Kernel-侧-DumpTensor"><a href="#3-Kernel-侧-DumpTensor" class="headerlink" title="3. Kernel 侧 DumpTensor"></a>3. Kernel 侧 DumpTensor</h5><p>当需要调试 <strong>GlobalTensor</strong> 或 <strong>LocalTensor</strong> 时，可使用 <code>DumpTensor</code> 输出中间张量的数据到文件。该功能需要在编译及配置文件中进行相应配置：</p>
<ul>
<li>编译时开关：在 <code>op_kernel/CMakeLists.txt</code> 中增加：</li>
</ul>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">add_ops_compile_options(ALL OPTIONS -DASCENDC_DUMP)</span><br></pre></td></tr></table></figure>

<ul>
<li>配置文件：编辑 <code>AclNNInvocation/scripts/acl.json</code>，启用 <code>dump</code> 功能：</li>
</ul>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;dump&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;dump_path&quot;</span><span class="punctuation">:</span><span class="string">&quot;/home/HwHiAiUser/dump&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;dump_mode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;all&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;dump_debug&quot;</span><span class="punctuation">:</span> <span class="string">&quot;off&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;dump_op_switch&quot;</span><span class="punctuation">:</span> <span class="string">&quot;on&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>代码使用：在 Kernel 中调用</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DumpTensor(xLocal,73, 144);</span><br></pre></td></tr></table></figure>

<p>其中 <code>xLocal</code> 为需要 Dump 的张量句柄，<code>73</code>、<code>144</code> 可以是自定义标识，或张量的大小信息等参数。</p>
<hr>
<h5 id="4-查看日志"><a href="#4-查看日志" class="headerlink" title="4. 查看日志"></a>4. 查看日志</h5><p>有时可以通过日志查看关键信息或错误提示，常用日志路径为 <code>~/ascend/log/debug/plog</code>。可以使用 <code>grep</code> 进行过滤：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ~/ascend/log/debug/plog</span><br><span class="line">grep -rn <span class="string">&quot;error&quot;</span></span><br><span class="line">grep -rn <span class="string">&quot;errorStr&quot;</span></span><br></pre></td></tr></table></figure>

<hr>
<h5 id="5-编译器输出类型信息"><a href="#5-编译器输出类型信息" class="headerlink" title="5. 编译器输出类型信息"></a>5. 编译器输出类型信息</h5><p>如果需要在编译阶段确认某个宏或类型的值（如 <code>DTYPE_X</code>），可以通过预处理器和编译器的警告信息来实现。例如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#define STRINGIZE(x) #x</span><br><span class="line">#define TOSTRING(x) STRINGIZE(x)</span><br><span class="line"></span><br><span class="line">#pragma message(&quot;DTYPE_X is &quot; TOSTRING(DTYPE_X))</span><br></pre></td></tr></table></figure>

<p>编译器在编译时会在输出信息中提示 <code>DTYPE_X is xxx</code>，帮助我们在编译期确认类型或宏定义。</p>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">9-GirdSample算子预分析</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-01-22 15:10:11" itemprop="dateCreated datePublished" datetime="2025-01-22T15:10:11+08:00">2025-01-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-02-11 16:13:42" itemprop="dateModified" datetime="2025-02-11T16:13:42+08:00">2025-02-11</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>整理了从环境安装、ONNX 导出、OM 模型转换、推理验证到算子支持查询的完整流程，并针对 GridSampler2D&#x2F;3D 的关键问题和解决思路进行了说明。</p>
<hr>
<h5 id="1-参考资料"><a href="#1-参考资料" class="headerlink" title="1. 参考资料"></a>1. 参考资料</h5><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha003/apiref/aolapi/atlasoxol_09_0071.html">GridSample-支持ONNX算子清单-AI框架算子支持清单-算子加速库接口-CANN社区版8.0.0.alpha003开发文档-昇腾社区</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha003/devaids/devtools/atc/atlasatc_16_0003.html#ZH-CN_TOPIC_0000002115847872__section1645421681020">快速入门-ATC工具-训练&amp;推理开发-CANN社区版8.0.0.alpha003开发文档-昇腾社区</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha003/apiref/aolapi/operatorlist_00095.html">规格清单-CANN算子规格说明-算子加速库接口-CANN社区版8.0.0.alpha003开发文档-昇腾社区</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://gitee.com/ascend/tools/tree/master/ais-bench_workload/tool/ais_bench">tools: Ascend tools - Gitee.com</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/CANNCommunityEdition/800alpha003/devaids/devtools/opmodelquery/atlasopmodelquery_16_0003.html">操作指南-算子及模型速查工具-训练&amp;推理开发-CANN社区版8.0.0.alpha003开发文档-昇腾社区</a></p>
</li>
</ul>
<hr>
<h5 id="2-安装环境"><a href="#2-安装环境" class="headerlink" title="2. 安装环境"></a>2. 安装环境</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch&gt;=<span class="number">1.9</span> onnx==<span class="number">1.12</span><span class="number">.0</span> onnxruntime==<span class="number">1.14</span><span class="number">.0</span></span><br></pre></td></tr></table></figure>

<ul>
<li>Opset v16</li>
</ul>
<hr>
<h5 id="3-导出-2D-GridSample-的-ONNX-模型"><a href="#3-导出-2D-GridSample-的-ONNX-模型" class="headerlink" title="3. 导出 2D GridSample 的 ONNX 模型"></a>3. 导出 2D GridSample 的 ONNX 模型</h5><ul>
<li>编写导出脚本 <code>export_grid_sample_onnx.py</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim export_grid_sample_onnx.py</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnxruntime <span class="keyword">as</span> ort</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GridSampleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(GridSampleModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, </span><br><span class="line">                              kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, grid</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv(x)</span><br><span class="line"></span><br><span class="line">        out = F.grid_sample(</span><br><span class="line">            x, </span><br><span class="line">            grid, </span><br><span class="line">            mode=<span class="string">&#x27;bilinear&#x27;</span>, </span><br><span class="line">            padding_mode=<span class="string">&#x27;zeros&#x27;</span>, </span><br><span class="line">            align_corners=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    model = GridSampleModel().<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    N, C, H, W = <span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span></span><br><span class="line">    x = torch.randn(N, C, H, W)</span><br><span class="line"></span><br><span class="line">    theta = torch.tensor([</span><br><span class="line">        [[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line">         [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>]]</span><br><span class="line">    ], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">    grid = F.affine_grid(theta, size=(N, C, H, W), align_corners=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        out = model(x, grid)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PyTorch output shape:&quot;</span>, out.shape)</span><br><span class="line"></span><br><span class="line">    onnx_model_path = <span class="string">&quot;grid_sample_model.onnx&quot;</span></span><br><span class="line">    torch.onnx.export(</span><br><span class="line">        model,</span><br><span class="line">        (x, grid),</span><br><span class="line">        onnx_model_path,</span><br><span class="line">        export_params=<span class="literal">True</span>,</span><br><span class="line">        opset_version=<span class="number">16</span>,</span><br><span class="line">        do_constant_folding=<span class="literal">True</span>,</span><br><span class="line">        input_names=[<span class="string">&quot;input_x&quot;</span>, <span class="string">&quot;input_grid&quot;</span>],</span><br><span class="line">        output_names=[<span class="string">&quot;output&quot;</span>],</span><br><span class="line">        dynamic_axes=&#123;</span><br><span class="line">            <span class="string">&quot;input_x&quot;</span>: &#123;<span class="number">0</span>: <span class="string">&quot;batch_size&quot;</span>, <span class="number">2</span>: <span class="string">&quot;height&quot;</span>, <span class="number">3</span>: <span class="string">&quot;width&quot;</span>&#125;,</span><br><span class="line">            <span class="string">&quot;input_grid&quot;</span>: &#123;<span class="number">0</span>: <span class="string">&quot;batch_size&quot;</span>, <span class="number">1</span>: <span class="string">&quot;grid_height&quot;</span>, <span class="number">2</span>: <span class="string">&quot;grid_width&quot;</span>&#125;,</span><br><span class="line">            <span class="string">&quot;output&quot;</span>: &#123;<span class="number">0</span>: <span class="string">&quot;batch_size&quot;</span>, <span class="number">2</span>: <span class="string">&quot;out_height&quot;</span>, <span class="number">3</span>: <span class="string">&quot;out_width&quot;</span>&#125;</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;ONNX model saved to: <span class="subst">&#123;onnx_model_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">    onnx_model = onnx.load(onnx_model_path)</span><br><span class="line">    onnx.checker.check_model(onnx_model)</span><br><span class="line"></span><br><span class="line">    ort_sess = ort.InferenceSession(onnx_model_path)</span><br><span class="line"></span><br><span class="line">    x_np = x.cpu().numpy()</span><br><span class="line">    grid_np = grid.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    onnx_out = ort_sess.run(</span><br><span class="line">        <span class="literal">None</span>,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;input_x&quot;</span>: x_np,</span><br><span class="line">            <span class="string">&quot;input_grid&quot;</span>: grid_np</span><br><span class="line">        &#125;</span><br><span class="line">    )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;ONNX Runtime output shape:&quot;</span>, onnx_out.shape)</span><br><span class="line"></span><br><span class="line">    diff = np.mean(np.<span class="built_in">abs</span>(out.cpu().numpy() - onnx_out))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Mean absolute difference between PyTorch and ONNX outputs: <span class="subst">&#123;diff:<span class="number">.6</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<ul>
<li>运行导出脚本</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 export_grid_sample.py</span><br></pre></td></tr></table></figure>

<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-12-06-25-image.png"></p>
<hr>
<h5 id="4-转换成om模型"><a href="#4-转换成om模型" class="headerlink" title="4. 转换成om模型"></a>4. 转换成om模型</h5><p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-12-54-10-image.png"></p>
<ul>
<li>使用 ATC 工具将 ONNX 模型转换为 OM 模型：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">atc --model=grid_sample_model.onnx --framework=5 --output=grid_sample_model --input_shape=<span class="string">&quot;input_x:1,1,64,64;input_grid:1,64,64,2&quot;</span> --soc_version=Ascend910B3</span><br></pre></td></tr></table></figure>

<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-12-21-45-image.png"></p>
<hr>
<h5 id="5-开始推理验证"><a href="#5-开始推理验证" class="headerlink" title="5. 开始推理验证"></a>5. 开始推理验证</h5><ul>
<li>安装ais_bench推理工具：从<a target="_blank" rel="noopener" href="https://gitee.com/ascend/tools/tree/master/ais-bench_workload/tool/ais_bench">tools: Ascend tools - Gitee.com</a>，通过源代码编译安装</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> tools-master/ais-bench_workload/tool/ais_bench</span><br><span class="line">pip3 wheel ./backend/ -v</span><br><span class="line">pip3 wheel ./ -v</span><br><span class="line">pip3 install ./aclruntime-0.0.2-cp311-cp311-linux_aarch64.whl --force-reinstall</span><br><span class="line">pip3 install --force-reinstall <span class="string">&quot;numpy&lt;2.0&quot;</span></span><br><span class="line">pip3 install --no-deps ./ais_bench-0.0.2-py3-none-any.whl --force-reinstall</span><br><span class="line">pip3 install ./ais_bench-0.0.2-py3-none-any.whl --force-reinstall</span><br></pre></td></tr></table></figure>

<ul>
<li>生成输入数据<code>generate_bin.py</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> prep_dataset</span><br><span class="line"><span class="built_in">cd</span> prep_dataset</span><br><span class="line">vim generate_bin.py</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    N, C, H, W = <span class="number">1</span>, <span class="number">1</span>, <span class="number">64</span>, <span class="number">64</span></span><br><span class="line">    x = torch.randn(N, C, H, W, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">    theta = torch.tensor([</span><br><span class="line">        [[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line">         [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>]]</span><br><span class="line">    ], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">    grid = F.affine_grid(theta, size=(N, C, H, W), align_corners=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    x_np = x.cpu().numpy()</span><br><span class="line">    grid_np = grid.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    x_np.tofile(<span class="string">&quot;input_x.bin&quot;</span>)</span><br><span class="line">    grid_np.tofile(<span class="string">&quot;input_grid.bin&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Generated two .bin files:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot; - input_x.bin    : shape <span class="subst">&#123;x_np.shape&#125;</span>, dtype <span class="subst">&#123;x_np.dtype&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot; - input_grid.bin : shape <span class="subst">&#123;grid_np.shape&#125;</span>, dtype <span class="subst">&#123;grid_np.dtype&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>运行脚本生成二进制输入文件，并创建相应目录：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python3 generate_bin.py</span><br><span class="line"><span class="built_in">mkdir</span> input_x_dir</span><br><span class="line"><span class="built_in">mkdir</span> input_grid_dir</span><br><span class="line"><span class="built_in">mv</span> input_x.bin input_x_dir/</span><br><span class="line"><span class="built_in">mv</span> input_grid.bin input_grid_dir/</span><br></pre></td></tr></table></figure>

<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-13-40-43-image.png"></p>
<ul>
<li>使用 ais_bench 进行推理</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line"><span class="built_in">source</span> /usr/local/Ascend/ascend-toolkit/set_env.sh</span><br><span class="line">python -m ais_bench --model grid_sample_model.om</span><br><span class="line"><span class="built_in">mkdir</span> output</span><br><span class="line">python3 -m ais_bench --model ./grid_sample_model.om --input ./prep_dataset/input_x_dir,./prep_dataset/input_grid_dir --output ./output/ --output_dirname result --outfmt TXT</span><br></pre></td></tr></table></figure>

<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-13-32-18-image.png"></p>
<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-13-58-11-image.png"></p>
<hr>
<h5 id="6-更多性能数据"><a href="#6-更多性能数据" class="headerlink" title="6. 更多性能数据"></a>6. 更多性能数据</h5><p>创建或编辑 <code>acl.json</code> 文件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim acl.json</span><br></pre></td></tr></table></figure>

<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;profiler&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">              <span class="attr">&quot;switch&quot;</span><span class="punctuation">:</span> <span class="string">&quot;on&quot;</span><span class="punctuation">,</span></span><br><span class="line">              <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;./result/profiler&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>执行推理并采集性能数据：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m ais_bench --model ./grid_sample_model.om --acl_json_path ./acl.json</span><br></pre></td></tr></table></figure>

<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-14-03-38-image.png"></p>
<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-14-03-57-image.png"></p>
<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-14-53-31-0fe95443ce374c44f2a10a4e775a91a.png"></p>
<hr>
<h5 id="7-算子支持与注意事项"><a href="#7-算子支持与注意事项" class="headerlink" title="7. 算子支持与注意事项"></a>7. 算子支持与注意事项</h5><ul>
<li>查看CANN算子规格</li>
</ul>
<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-21-01-15-31-image.png"></p>
<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-21-01-14-58-image.png"></p>
<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-20-16-11-image.png"></p>
<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-20-16-26-image.png"></p>
<p>如果使用GridSampler3D：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim export_grid_sample_3D.py</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GridSample3DModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(GridSample3DModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.conv3d = nn.Conv3d(in_channels=<span class="number">1</span>, out_channels=<span class="number">1</span>, </span><br><span class="line">                                kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, grid</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.conv3d(x)</span><br><span class="line">        out = F.grid_sample(</span><br><span class="line">            x, </span><br><span class="line">            grid, </span><br><span class="line">            mode=<span class="string">&#x27;bilinear&#x27;</span>,</span><br><span class="line">            padding_mode=<span class="string">&#x27;zeros&#x27;</span>,</span><br><span class="line">            align_corners=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    model = GridSample3DModel().<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    N, C, D, H, W = <span class="number">1</span>, <span class="number">1</span>, <span class="number">8</span>, <span class="number">640</span>, <span class="number">640</span></span><br><span class="line">    x = torch.randn(N, C, D, H, W, dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">    theta = torch.tensor([[</span><br><span class="line">        [<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line">        [<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line">        [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>],</span><br><span class="line">    ]], dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">    grid = F.affine_grid(theta, size=(N, C, D, H, W), align_corners=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        out = model(x, grid)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;PyTorch output shape:&quot;</span>, out.shape)</span><br><span class="line"></span><br><span class="line">    onnx_model_path = <span class="string">&quot;grid_sample_3d_model.onnx&quot;</span></span><br><span class="line">    torch.onnx.export(</span><br><span class="line">        model,</span><br><span class="line">        (x, grid),</span><br><span class="line">        onnx_model_path,</span><br><span class="line">        export_params=<span class="literal">True</span>,</span><br><span class="line">        opset_version=<span class="number">16</span>,</span><br><span class="line">        do_constant_folding=<span class="literal">True</span>,</span><br><span class="line">        input_names=[<span class="string">&quot;input_x&quot;</span>, <span class="string">&quot;input_grid&quot;</span>],</span><br><span class="line">        output_names=[<span class="string">&quot;output&quot;</span>],</span><br><span class="line">        <span class="comment"># dynamic_axes=&#123;</span></span><br><span class="line">        <span class="comment">#   &quot;input_x&quot;: &#123;0: &quot;batch_size&quot;, 2: &quot;depth&quot;, 3: &quot;height&quot;, 4: &quot;width&quot;&#125;,</span></span><br><span class="line">        <span class="comment">#   &quot;input_grid&quot;: &#123;0: &quot;batch_size&quot;, 1: &quot;grid_depth&quot;, 2: &quot;grid_height&quot;, 3: &quot;grid_width&quot;&#125;,</span></span><br><span class="line">        <span class="comment">#   &quot;output&quot;: &#123;0: &quot;batch_size&quot;, 2: &quot;out_depth&quot;, 3: &quot;out_height&quot;, 4: &quot;out_width&quot;&#125;</span></span><br><span class="line">        <span class="comment"># &#125;</span></span><br><span class="line">    )</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;ONNX model saved to: <span class="subst">&#123;onnx_model_path&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 export_grid_sample_3D.py</span><br></pre></td></tr></table></figure>

<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-20-28-05-image.png"></p>
<ul>
<li><p>GridSampler2D 与 GridSampler3D 的限制</p>
<ul>
<li><p><strong>PyTorch ONNX 导出限制</strong>：目前 <code>torch.onnx.export</code> 仅支持 2D GridSample（4D 输入），不支持 3D（5D）输入，会报错。</p>
</li>
<li><p><strong>Ascend AI Core 支持</strong>：</p>
<ul>
<li>GridSampler3D 有 AI Core Kernel 实现，需 float32 前向；</li>
<li>GridSampler2D 目前仅支持在 AICPU 上执行，无法使用 AI Core。</li>
</ul>
</li>
</ul>
</li>
<li><p>3D grid_sample 的计算量通常远大于 2D，如果本来只是个 2D 任务，用 3D 人工加一维度可能并不能带来真正的功能收益，只是为了能够在 AI Core上执行。</p>
</li>
<li><p>这是一个前向支持 &#x2F; 后向不支持的状况。如果要在 Ascend AI Core 上做推理，又通过 ONNX&#x2F;OM 流程，目前<strong>没有官方开箱可行的方案</strong>。</p>
</li>
</ul>
<hr>
<h5 id="8-解决思路："><a href="#8-解决思路：" class="headerlink" title="8. 解决思路："></a>8. 解决思路：</h5><p>面对上述限制，以下是可能的应对思路：</p>
<ul>
<li><p><strong>使用 2D GridSample</strong>：如果不强制使用 AI Core 加速，可使用 2D GridSample，会在 AICPU 上运行，性能较低但流程简单。</p>
</li>
<li><p><strong>直接在 PyTorch + Ascend NPU 上推理</strong>：跳过 ONNX&#x2F;OM 转换，使用 <code>torch_npu</code> 在 Ascend NPU 上直接运行模型。</p>
</li>
<li><p><strong>使用 GridSampler3D</strong>：</p>
<ul>
<li><p>构造 5D 输入 <code>x</code>（形状 <code>[N, C, D, H, W]</code>，dtype float32）和 5D 网格 <code>grid</code>（形状 <code>[N, D, H, W, 3]</code>，dtype float32）。</p>
</li>
<li><p>在计算图中添加 GridSampler3D 节点，并设置相关属性。</p>
</li>
<li><p>使用 ATC 或其他编译器，将计算图编译为 <code>.om</code> 文件，在 AI Core 上执行。注意这种方式需要深入了解 Ascend 图编译流程。</p>
</li>
</ul>
</li>
<li><p><strong>自定义实现</strong>：自行实现 3D GridSample 的 ONNX symbolic 函数或 Ascend 自定义算子，但工作量较大。</p>
</li>
</ul>
<hr>
<h5 id="9-查询算子支持"><a href="#9-查询算子支持" class="headerlink" title="9. 查询算子支持"></a>9. 查询算子支持</h5><ul>
<li>使用 <code>ms_fast_query</code> 工具查询算子支持情况：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/Ascend/ascend-toolkit/latest/tools/ms_fast_query</span><br><span class="line">python3 ms_fast_query.py -t op --opp_path /usr/local/Ascend/ascend-toolkit/latest/opp -o /home/ljw/op.json</span><br></pre></td></tr></table></figure>

<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-21-11-05-image.png"></p>
<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-20-21-11-20-image.png"></p>
<p>查询结果会生成在指定的 <code>op.json</code> 文件中，可用于查看算子支持详情。</p>
<hr>
<h5 id="10-查看算子原型"><a href="#10-查看算子原型" class="headerlink" title="10. 查看算子原型"></a>10. 查看算子原型</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/Ascend/ascend-toolkit/latest/opp/built-in/op_proto/</span><br><span class="line"><span class="built_in">cat</span> image_ops.h</span><br></pre></td></tr></table></figure>

<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-21-01-04-19-image.png"></p>
<p><img src="/2025/01/22/9-GirdSample%E7%AE%97%E5%AD%90%E9%A2%84%E5%88%86%E6%9E%90/2025-01-21-01-06-12-image.png"></p>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/01/22/8-%E4%BD%BF%E7%94%A8CANN%E7%AE%97%E5%AD%90%E5%BA%93%E8%BF%9B%E8%A1%8C%E4%B8%A4%E6%AE%B5%E5%BC%8F%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Jiawei Li">
      <meta itemprop="description" content="愚蠢的zzz侠">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="睿智的ljw侠">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/01/22/8-%E4%BD%BF%E7%94%A8CANN%E7%AE%97%E5%AD%90%E5%BA%93%E8%BF%9B%E8%A1%8C%E4%B8%A4%E6%AE%B5%E5%BC%8F%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8/" class="post-title-link" itemprop="url">8-使用CANN算子库进行两段式接口调用</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-01-22 11:10:11" itemprop="dateCreated datePublished" datetime="2025-01-22T11:10:11+08:00">2025-01-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-01-23 11:51:37" itemprop="dateModified" datetime="2025-01-23T11:51:37+08:00">2025-01-23</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本教程指导如何使用 AscendCL 和 CANN 算子库接口 <code>aclnnGridSampler2D</code> 进行 2D 网格采样操作。</p>
<hr>
<h5 id="1-参考"><a href="#1-参考" class="headerlink" title="1. 参考"></a>1. 参考</h5><ul>
<li><p><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/canncommercial/800/apiref/aolapi/context/aclnnGridSampler2D.md">aclnnGridSampler2D-NN算子接口-算子加速库-CANN商用版8.0.0开发文档-昇腾社区</a></p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.hiascend.com/document/detail/zh/canncommercial/800/developmentguide/appdevg/aclcppdevg/aclcppdevg_000019.html">调用NN&#x2F;融合算子接口示例代码-单算子API执行-单算子调用-AscendCL应用开发（C&amp;C++）-应用开发-CANN商用版8.0.0开发文档-昇腾社区</a></p>
</li>
</ul>
<hr>
<h5 id="2-示例代码"><a href="#2-示例代码" class="headerlink" title="2. 示例代码"></a>2. 示例代码</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim test_grid_sample2d.cpp</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;acl/acl.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;aclnnop/aclnn_grid_sampler2d.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_RET(cond, return_expr) \</span></span><br><span class="line"><span class="meta">  do &#123;                               \</span></span><br><span class="line"><span class="meta">    <span class="keyword">if</span> (!(cond)) &#123;                   \</span></span><br><span class="line"><span class="meta">      return_expr;                   \</span></span><br><span class="line"><span class="meta">    &#125;                                \</span></span><br><span class="line"><span class="meta">  &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> LOG_PRINT(message, ...)     \</span></span><br><span class="line"><span class="meta">  do &#123;                              \</span></span><br><span class="line"><span class="meta">    printf(message, ##__VA_ARGS__); \</span></span><br><span class="line"><span class="meta">  &#125; while (0)</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int64_t</span> <span class="title">GetShapeSize</span><span class="params">(<span class="type">const</span> std::vector&lt;<span class="type">int64_t</span>&gt;&amp; shape)</span> </span>&#123;</span><br><span class="line">  <span class="type">int64_t</span> shapeSize = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span> i : shape) &#123;</span><br><span class="line">    shapeSize *= i;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> shapeSize;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">Init</span><span class="params">(<span class="type">int32_t</span> deviceId, aclrtStream* stream)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 固定写法，AscendCL初始化</span></span><br><span class="line">  <span class="keyword">auto</span> ret = <span class="built_in">aclInit</span>(<span class="literal">nullptr</span>);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;aclInit failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line">  ret = <span class="built_in">aclrtSetDevice</span>(deviceId);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;aclrtSetDevice failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line">  ret = <span class="built_in">aclrtCreateStream</span>(stream);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;aclrtCreateStream failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">CreateAclTensor</span><span class="params">(<span class="type">const</span> std::vector&lt;T&gt;&amp; hostData, <span class="type">const</span> std::vector&lt;<span class="type">int64_t</span>&gt;&amp; shape, <span class="type">void</span>** deviceAddr,</span></span></span><br><span class="line"><span class="params"><span class="function">                    aclDataType dataType, aclTensor** tensor)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> size = <span class="built_in">GetShapeSize</span>(shape) * <span class="built_in">sizeof</span>(T);</span><br><span class="line">  <span class="comment">// 调用aclrtMalloc申请device侧内存</span></span><br><span class="line">  <span class="keyword">auto</span> ret = <span class="built_in">aclrtMalloc</span>(deviceAddr, size, ACL_MEM_MALLOC_HUGE_FIRST);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;aclrtMalloc failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line">  <span class="comment">// 调用aclrtMemcpy将host侧数据复制到device侧内存上</span></span><br><span class="line">  ret = <span class="built_in">aclrtMemcpy</span>(*deviceAddr, size, hostData.<span class="built_in">data</span>(), size, ACL_MEMCPY_HOST_TO_DEVICE);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;aclrtMemcpy failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算连续tensor的strides</span></span><br><span class="line">  <span class="function">std::vector&lt;<span class="type">int64_t</span>&gt; <span class="title">strides</span><span class="params">(shape.size(), <span class="number">1</span>)</span></span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int64_t</span> i = shape.<span class="built_in">size</span>() - <span class="number">2</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">    strides[i] = shape[i + <span class="number">1</span>] * strides[i + <span class="number">1</span>];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 调用aclCreateTensor接口创建aclTensor</span></span><br><span class="line">  *tensor = <span class="built_in">aclCreateTensor</span>(shape.<span class="built_in">data</span>(), shape.<span class="built_in">size</span>(), dataType, strides.<span class="built_in">data</span>(), <span class="number">0</span>, aclFormat::ACL_FORMAT_ND,</span><br><span class="line">                            shape.<span class="built_in">data</span>(), shape.<span class="built_in">size</span>(), *deviceAddr);</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 1. （固定写法）device/stream初始化，参考AscendCL对外接口列表</span></span><br><span class="line">  <span class="comment">// 根据自己的实际device填写deviceId</span></span><br><span class="line">  <span class="type">int32_t</span> deviceId = <span class="number">0</span>;</span><br><span class="line">  aclrtStream stream;</span><br><span class="line">  <span class="keyword">auto</span> ret = <span class="built_in">Init</span>(deviceId, &amp;stream);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;Init acl failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 2. 构造输入与输出，需要根据API的接口自定义构造</span></span><br><span class="line">  <span class="type">int64_t</span> interpolationMode = <span class="number">0</span>;</span><br><span class="line">  <span class="type">int64_t</span> paddingMode = <span class="number">0</span>;</span><br><span class="line">  <span class="type">bool</span> alignCorners = <span class="literal">false</span>;</span><br><span class="line">  std::vector&lt;<span class="type">int64_t</span>&gt; inputShape = &#123;<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">8</span>&#125;;</span><br><span class="line">  std::vector&lt;<span class="type">int64_t</span>&gt; gridShape = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>&#125;;</span><br><span class="line">  std::vector&lt;<span class="type">int64_t</span>&gt; outShape = &#123;<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>&#125;;</span><br><span class="line">  <span class="type">void</span>* inputDeviceAddr = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="type">void</span>* gridDeviceAddr = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="type">void</span>* outDeviceAddr = <span class="literal">nullptr</span>;</span><br><span class="line">  aclTensor* input = <span class="literal">nullptr</span>;</span><br><span class="line">  aclTensor* grid = <span class="literal">nullptr</span>;</span><br><span class="line">  aclTensor* out = <span class="literal">nullptr</span>;</span><br><span class="line"></span><br><span class="line">  std::vector&lt;<span class="type">float</span>&gt; inputHostData = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>, <span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>,</span><br><span class="line">                                      <span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>, <span class="number">30</span>, <span class="number">31</span>, <span class="number">32</span>, <span class="number">33</span>, <span class="number">34</span>, <span class="number">35</span>, <span class="number">36</span>, <span class="number">37</span>, <span class="number">38</span>, <span class="number">39</span>, <span class="number">40</span>&#125;;</span><br><span class="line">  std::vector&lt;<span class="type">float</span>&gt; gridHostData = &#123;<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">  std::vector&lt;<span class="type">float</span>&gt; outHostData = &#123;<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 创建input aclTensor</span></span><br><span class="line">  ret = <span class="built_in">CreateAclTensor</span>(inputHostData, inputShape, &amp;inputDeviceAddr, aclDataType::ACL_FLOAT, &amp;input);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="keyword">return</span> ret);</span><br><span class="line">  <span class="comment">// 创建grid aclTensor</span></span><br><span class="line">  ret = <span class="built_in">CreateAclTensor</span>(gridHostData, gridShape, &amp;gridDeviceAddr, aclDataType::ACL_FLOAT, &amp;grid);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="keyword">return</span> ret);</span><br><span class="line">  <span class="comment">// 创建out aclTensor</span></span><br><span class="line">  ret = <span class="built_in">CreateAclTensor</span>(outHostData, outShape, &amp;outDeviceAddr, aclDataType::ACL_FLOAT, &amp;out);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="keyword">return</span> ret);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 3. 调用CANN算子库API，需要修改为具体的Api名称</span></span><br><span class="line">  <span class="type">uint64_t</span> workspaceSize = <span class="number">0</span>;</span><br><span class="line">  aclOpExecutor* executor;</span><br><span class="line">  <span class="comment">// 调用aclnnGridSampler2D第一段接口</span></span><br><span class="line">  ret = <span class="built_in">aclnnGridSampler2DGetWorkspaceSize</span>(input, grid, interpolationMode, paddingMode,alignCorners, out,</span><br><span class="line">                                           &amp;workspaceSize, &amp;executor);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;aclnnGridSampler2DGetWorkspaceSize failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line">  <span class="comment">// 根据第一段接口计算出的workspaceSize申请device内存</span></span><br><span class="line">  <span class="type">void</span>* workspaceAddr = <span class="literal">nullptr</span>;</span><br><span class="line">  <span class="keyword">if</span> (workspaceSize &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    ret = <span class="built_in">aclrtMalloc</span>(&amp;workspaceAddr, workspaceSize, ACL_MEM_MALLOC_HUGE_FIRST);</span><br><span class="line">    <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;allocate workspace failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 调用aclnnGridSampler2D第二段接口</span></span><br><span class="line">  ret = <span class="built_in">aclnnGridSampler2D</span>(workspaceAddr, workspaceSize, executor, stream);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;aclnnGridSampler2D failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 4. （固定写法）同步等待任务执行结束</span></span><br><span class="line">  ret = <span class="built_in">aclrtSynchronizeStream</span>(stream);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;aclrtSynchronizeStream failed. ERROR: %d\n&quot;</span>, ret); <span class="keyword">return</span> ret);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 5. 获取输出的值，将device侧内存上的结果复制至host侧，需要根据具体API的接口定义修改</span></span><br><span class="line">  <span class="keyword">auto</span> size = <span class="built_in">GetShapeSize</span>(outShape);</span><br><span class="line">  <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">resultData</span><span class="params">(size, <span class="number">0</span>)</span></span>;</span><br><span class="line">  ret = <span class="built_in">aclrtMemcpy</span>(resultData.<span class="built_in">data</span>(), resultData.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(resultData[<span class="number">0</span>]),</span><br><span class="line">                    outDeviceAddr, size * <span class="built_in">sizeof</span>(resultData[<span class="number">0</span>]), ACL_MEMCPY_DEVICE_TO_HOST);</span><br><span class="line">  <span class="built_in">CHECK_RET</span>(ret == ACL_SUCCESS, <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;copy resultData from device to host failed. ERROR: %d\n&quot;</span>, ret);</span><br><span class="line">            <span class="keyword">return</span> ret);</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int64_t</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">    <span class="built_in">LOG_PRINT</span>(<span class="string">&quot;resultData[%ld] is: %f\n&quot;</span>, i, resultData[i]);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 6. 释放aclTensor，需要根据具体API的接口定义修改</span></span><br><span class="line">  <span class="built_in">aclDestroyTensor</span>(input);</span><br><span class="line">  <span class="built_in">aclDestroyTensor</span>(grid);</span><br><span class="line">  <span class="built_in">aclDestroyTensor</span>(out);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 7. 释放Device资源，需要根据具体API的接口定义修改</span></span><br><span class="line">  <span class="built_in">aclrtFree</span>(inputDeviceAddr);</span><br><span class="line">  <span class="built_in">aclrtFree</span>(gridDeviceAddr);</span><br><span class="line">  <span class="built_in">aclrtFree</span>(outDeviceAddr);</span><br><span class="line">  <span class="keyword">if</span> (workspaceSize &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">aclrtFree</span>(workspaceAddr);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">aclrtDestroyStream</span>(stream);</span><br><span class="line">  <span class="built_in">aclrtResetDevice</span>(deviceId);</span><br><span class="line">  <span class="built_in">aclFinalize</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="3-CMakeLists配置"><a href="#3-CMakeLists配置" class="headerlink" title="3. CMakeLists配置"></a>3. CMakeLists配置</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim CMakeLists.txt</span><br></pre></td></tr></table></figure>

<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CMake lowest version requirement</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.14</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置工程名</span></span><br><span class="line"><span class="keyword">project</span>(ACLNN_EXAMPLE)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compile options</span></span><br><span class="line"><span class="keyword">add_compile_options</span>(-std=c++<span class="number">11</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置编译选项</span></span><br><span class="line"><span class="keyword">set</span>(CMAKE_RUNTIME_OUTPUT_DIRECTORY  <span class="string">&quot;./bin&quot;</span>)    </span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS_DEBUG <span class="string">&quot;-fPIC -O0 -g -Wall&quot;</span>)</span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS_RELEASE <span class="string">&quot;-fPIC -O2 -Wall&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置可执行文件名（如opapi_test），并指定待运行算子文件*.cpp所在目录</span></span><br><span class="line"><span class="keyword">add_executable</span>(opapi_test</span><br><span class="line">               test_grid_sample2d.cpp) </span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置ASCEND_PATH（CANN软件包目录，请根据实际路径修改）和INCLUDE_BASE_DIR（头文件目录）</span></span><br><span class="line"><span class="keyword">if</span>(<span class="keyword">NOT</span> <span class="string">&quot;$ENV&#123;ASCEND_CUSTOM_PATH&#125;&quot;</span> <span class="keyword">STREQUAL</span> <span class="string">&quot;&quot;</span>)      </span><br><span class="line">    <span class="keyword">set</span>(ASCEND_PATH $ENV&#123;ASCEND_CUSTOM_PATH&#125;)</span><br><span class="line"><span class="keyword">else</span>()</span><br><span class="line">    <span class="keyword">set</span>(ASCEND_PATH <span class="string">&quot;/usr/local/Ascend/ascend-toolkit/latest&quot;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"><span class="keyword">set</span>(INCLUDE_BASE_DIR <span class="string">&quot;$&#123;ASCEND_PATH&#125;/include&quot;</span>)</span><br><span class="line"><span class="keyword">include_directories</span>(</span><br><span class="line">    <span class="variable">$&#123;INCLUDE_BASE_DIR&#125;</span></span><br><span class="line">    <span class="variable">$&#123;INCLUDE_BASE_DIR&#125;</span>/aclnn</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置链接的库文件路径</span></span><br><span class="line"><span class="keyword">target_link_libraries</span>(opapi_test PRIVATE</span><br><span class="line">                      <span class="variable">$&#123;ASCEND_PATH&#125;</span>/lib64/libascendcl.so</span><br><span class="line">                      <span class="variable">$&#123;ASCEND_PATH&#125;</span>/lib64/libnnopbase.so</span><br><span class="line">                      <span class="variable">$&#123;ASCEND_PATH&#125;</span>/lib64/libopapi.so)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可执行文件在CMakeLists文件所在目录的bin目录下</span></span><br><span class="line"><span class="keyword">install</span>(TARGETS opapi_test DESTINATION <span class="variable">$&#123;CMAKE_RUNTIME_OUTPUT_DIRECTORY&#125;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h5 id="4-编译与运行"><a href="#4-编译与运行" class="headerlink" title="4. 编译与运行"></a>4. 编译与运行</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get install -y cmake</span><br><span class="line"><span class="built_in">source</span> /usr/local/Ascend/ascend-toolkit/set_env.sh</span><br><span class="line"><span class="built_in">mkdir</span> -p build </span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ../ -DCMAKE_CXX_COMPILER=g++ -DCMAKE_SKIP_RPATH=TRUE</span><br><span class="line">make</span><br><span class="line"><span class="built_in">cd</span> bin</span><br><span class="line">./opapi_test</span><br></pre></td></tr></table></figure>

<p><img src="/2025/01/22/8-%E4%BD%BF%E7%94%A8CANN%E7%AE%97%E5%AD%90%E5%BA%93%E8%BF%9B%E8%A1%8C%E4%B8%A4%E6%AE%B5%E5%BC%8F%E6%8E%A5%E5%8F%A3%E8%B0%83%E7%94%A8/2025-01-22-09-59-42-image.png"></p>
<hr>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jiawei Li"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">Jiawei Li</p>
  <div class="site-description" itemprop="description">愚蠢的zzz侠</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">18</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jiawei Li</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
